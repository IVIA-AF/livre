%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,11pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother


\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=2,mathnumfig]{sphinx}
\sphinxsetup{verbatimsep=2mm,
                  VerbatimColor={rgb}{.95,.95,.95},
                  VerbatimBorderColor={rgb}{.95,.95,.95},
                  pre_border-radius=3pt,
               }
\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}




% Page size
\setlength{\voffset}{-14mm}
\addtolength{\textheight}{16mm}

% Chapter title style
\usepackage{titlesec, blindtext, color}
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}

% So some large pictures won't get the full page
\renewcommand{\floatpagefraction}{.8}

\setcounter{tocdepth}{1}
% Use natbib's citation style, e.g. (Li and Smola, 16)
\usepackage{natbib}
\protected\def\sphinxcite{\citep}





% Remove top header
\setlength{\headheight}{13.6pt}
\makeatletter
    \fancypagestyle{normal}{
        \fancyhf{}
        \fancyfoot[LE,RO]{{\py@HeaderFamily\thepage}}
        \fancyfoot[LO]{{\py@HeaderFamily\nouppercase{\rightmark}}}
        \fancyfoot[RE]{{\py@HeaderFamily\nouppercase{\leftmark}}}
        \fancyhead[LE,RO]{{\py@HeaderFamily }}
     }
\makeatother
% Defines macros for code-blocks styling
\definecolor{d2lbookOutputCellBackgroundColor}{RGB}{255,255,255}
\definecolor{d2lbookOutputCellBorderColor}{rgb}{.85,.85,.85}
\def\diilbookstyleoutputcell
   {\sphinxcolorlet{VerbatimColor}{d2lbookOutputCellBackgroundColor}%
    \sphinxcolorlet{VerbatimBorderColor}{d2lbookOutputCellBorderColor}%
    \sphinxsetup{verbatimwithframe,verbatimborder=0.5pt}%
   }%
%
\definecolor{d2lbookInputCellBackgroundColor}{rgb}{.95,.95,.95}
\def\diilbookstyleinputcell
   {\sphinxcolorlet{VerbatimColor}{d2lbookInputCellBackgroundColor}%
    \sphinxsetup{verbatimwithframe=false,verbatimborder=0pt}%
   }%
% memo: as this mark-up uses macros not environments we have to reset all changed
%       settings at each input cell to not inherit those or previous output cell
% memo: Sphinx 5.1.0, 5.1.1 ignore verbatimwithframe Boolean, so for this
%       reason we added an extra verbatimborder=0pt above.



\title{IVIA-AF: Initiative pour la Vulgarisation de l'Intelligence Artificielle dans l'AFRIUE francophone}
\date{Aug 30, 2025}
\release{1.0.0\sphinxhyphen{}beta0}
\author{IVIA-AF Team}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
L’Afrique est un continent dynamique et en pleine croissance, abritant
54 nations. Près de la moitié de ces pays partagent le français comme
langue officielle, ce qui représente une richesse culturelle et
linguistique considérable. Cependant, alors que le domaine de
l’apprentissage automatique (machine learning) connaît des avancées
fulgurantes, la grande majorité des ressources et des publications sont
rédigées en anglais. Cette réalité crée un obstacle significatif pour
les pays francophones désireux d’accéder à ces connaissances cruciales
et d’y apporter leurs propres contributions.

\sphinxAtStartPar
C’est pour lever cette barrière que nous avons entrepris de créer ce
cours d’introduction de niveau intermédiaire, entièrement en français.
Notre objectif est de rendre l’apprentissage automatique plus accessible
aux chercheurs et praticiens francophones, en leur offrant une ressource
complète et pertinente dans leur langue maternelle. Nous espérons ainsi
stimuler l’innovation et le développement de l’IA à travers le continent
africain et au\sphinxhyphen{}delà.


\chapter{Contribuez à notre projet !}
\label{\detokenize{index:contribuez-a-notre-projet}}
\sphinxAtStartPar
Ce projet \sphinxurl{https://github.com/IVIA-AF/livre} est une initiative
collaborative, et nous croyons fermement que la diversité des
perspectives est essentielle pour enrichir la connaissance. C’est
pourquoi nous invitons chaleureusement tout chercheur et praticien
francophone en intelligence artificielle à se joindre à nous. Que vous
souhaitiez suggérer des modifications, proposer des ajouts pour couvrir
des sujets supplémentaires, ou partager votre expertise, votre
contribution est précieuse. Ensemble, nous pouvons construire une
ressource d’apprentissage automatique francophone de premier plan.

\sphinxAtStartPar
Pour nous faire part de vos suggestions ou pour toute question,
n’hésitez pas à nous contacter à \sphinxhref{mailto:contact@ivia.africa}{contact@ivia.africa}.


\chapter{Table des matières}
\label{\detokenize{index:table-des-matieres}}
\sphinxstepscope


\section{Introduction Générale à l’Apprentissage Automatique}
\label{\detokenize{chapter1:introduction-generale-a-lapprentissage-automatique}}\label{\detokenize{chapter1::doc}}
\sphinxAtStartPar
Nous parlerons de:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Apprentissage Supervisé

\item {} 
\sphinxAtStartPar
Apprentissage Non\sphinxhyphen{}Supervisé

\item {} 
\sphinxAtStartPar
Les méthodes à noyaux (Kernel methods)

\item {} 
\sphinxAtStartPar
Apprentissage par Renforcement

\end{itemize}

\sphinxAtStartPar
Motivations et les applications pour chaque type d’apprentissage.


\subsection{C’est quoi Apprentissage Automatique?}
\label{\detokenize{chapter1:cest-quoi-apprentissage-automatique}}
\sphinxAtStartPar
Fig. {[}1.1{]}.

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{Categories-of-Machine-Learning}.jpg}
\caption{Les types d’I.A}\label{\detokenize{chapter1:id1}}\label{\detokenize{chapter1:type-ai}}\end{figure}


\subsection{Convention Mathématiques pour le document}
\label{\detokenize{chapter1:convention-mathematiques-pour-le-document}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Les matrices seront notées en lettre \sphinxstylestrong{majuscule} et seront mises en
\sphinxstylestrong{gras}. Par exemple,

\sphinxAtStartPar
\(\mathbf{X}\)

\item {} 
\sphinxAtStartPar
Les vecteurs seront notés en lettre \sphinxstylestrong{miniscule} et mises en
\sphinxstylestrong{gras}. Par exemple,

\sphinxAtStartPar
\(\mathbf{x}\).

\item {} 
\sphinxAtStartPar
Lécriture mathématique de probabilités, espérance seront
respectivement:

\sphinxAtStartPar
\(\mathbb{P}\), \(\mathbb{E}\)

\item {} 
\sphinxAtStartPar
Il sera aussi important de ponctuer les équations.

\item {} 
\sphinxAtStartPar
Numéroter les équations principales.

\item {} 
\sphinxAtStartPar
Tous les ensemble seront notés en utilisant

\sphinxAtStartPar
\(\mathbb{R}\)

\sphinxAtStartPar
par example.

\item {} 
\sphinxAtStartPar
les expressions mathématiques qui sont écrites à travers les textes
seront écrites dans

\sphinxAtStartPar
\(\operatorname{Prob}\)

\item {} 
\sphinxAtStartPar
Si c’est un symbole qui est un vecteur, on écrit (par exemple, si
c’est alpha)

\sphinxAtStartPar
\(\boldsymbol{\alpha}\)

\sphinxAtStartPar
par exemple.

\end{itemize}


\bigskip\hrule\bigskip



\subsection{Commentaires et Discussions}
\label{\detokenize{chapter1:commentaires-et-discussions}}
\sphinxAtStartPar
Partagez vos questions, commentaires et expériences avec la communauté
IVIA\sphinxhyphen{}AF ! Utilisez la version en ligne
\sphinxhref{https://livre.ivia.africa/chapter1.html}{livre.ivia.africa}%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxnolinkurl{https://livre.ivia.africa/chapter1.html}
%
\end{footnote}.

\sphinxAtStartPar
\sphinxstyleemphasis{Les commentaires sont modérés pour maintenir un environnement
d’apprentissage respectueux et constructif.}

\sphinxstepscope


\section{Pré\sphinxhyphen{}requis}
\label{\detokenize{chapter2:pre-requis}}\label{\detokenize{chapter2::doc}}
\sphinxAtStartPar
Python est le langage de programmation préféré des Data Scientistes. Ils
ont besoin d’un langage facile à utiliser, avec une disponibilité
décente des bibliothèques et une grande communauté. Les projets ayant
des communautés inactives sont généralement moins susceptibles de mettre
à jour leurs plates\sphinxhyphen{}formes. Mais alors, pourquoi Python est populaire en
Data Science ?

\sphinxAtStartPar
Python est connu depuis longtemps comme un langage de programmation
simple à maîtriser, du point de vue de la syntaxe. Python possède
également une communauté active et un vaste choix de bibliothèques et de
ressources. Comme résultat, vous disposez d’une plate\sphinxhyphen{}forme de
programmation qui est logique d’utiliser avec les technologies
émergentes telles que l’apprentissage automatique (Machine Learning) et
la Data Science.


\subsection{Langage Python et ses Librairies}
\label{\detokenize{chapter2:langage-python-et-ses-librairies}}
\sphinxAtStartPar
Python est un langage de programmation puissant et facile à apprendre.
Il dispose de structures de données de haut niveau et permet une
approche simple mais efficace de la programmation orientée objet. Parce
que sa syntaxe est élégante, que son typage est dynamique et qu’il est
interprété, Python est un langage idéal pour l’écriture de scripts quand
on fait de l’apprentissage automatique et le développement rapide
d’applications dans de nombreux domaines et sur la plupart des
plate\sphinxhyphen{}formes.


\subsubsection{Installation de Python et Anaconda}
\label{\detokenize{chapter2:installation-de-python-et-anaconda}}
\sphinxAtStartPar
L’installation de Python peut\sphinxhyphen{}être un vrai challenge. Déjà il faut se
décider entre les versions 2.X et 3.X du langage, par la suite, choisir
les librairies nécessaires (ainsi que les versions compatibles) pour
faire de l’apprentissage automatique (Machine Learning); sans oublier
les subtilités liées aux différents Systèmes d’exploitation (Windows,
Linux, Mac…) qui peuvent rendre l’installation encore plus
\sphinxstyleemphasis{“douloureuse”}.

\sphinxAtStartPar
Dans cette partie nous allons installer pas à pas un environnement de
développement Python en utilisant Anaconda{[}\textasciicircum{}1{]}. A l’issue de cette
partie, nous aurons un environnement de développement fonctionnel avec
les librairies (packages) nécessaires pour faire de l’apprentissage
automatique (Machine Learning).

\sphinxAtStartPar
\sphinxstylestrong{Qu’est ce que Anaconda ?}

\sphinxAtStartPar
L’installation d’un environnement Python complet peut\sphinxhyphen{}être assez
complexe. Déjà, il faut télécharger Python et l’installer, puis
télécharger une à une les librairies (packages) dont on a besoin.
Parfois, le nombre de ces librairies peut\sphinxhyphen{}être grand. Par ailleurs, il
faut s’assurer de la compatibilité entre les versions des différents
packages qu’on a à télécharger. \sphinxstyleemphasis{Bref, ce n’est pas amusant}!

\sphinxAtStartPar
Alors Anaconda est une distribution Python. À son installation, Anaconda
installera Python ainsi qu’une multitude de packages dont vous pouvez
consulter la
\sphinxhref{https://docs.anaconda.com/anaconda/packages/pkg-docs/\#Python-3-7}{liste}%
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.anaconda.com/anaconda/packages/pkg-docs/\#Python-3-7}
%
\end{footnote}.
Cela nous évite de nous ruer dans les problèmes d’incompatibilités entre
les différents packages. Finalement, Anaconda propose un outil de
gestion de packages appelé Conda. Ce dernier permettra de mettre à jour
et installer facilement les librairies dont on aura besoin pour nos
développements.

\sphinxAtStartPar
\sphinxstylestrong{Téléchargement et Installation de Anaconda}

\sphinxAtStartPar
\sphinxstylestrong{Note}: Les instructions qui suivent ont été testées sur Linux/Debian.
Le même processus d’installation pourra s’appliquer pour les autres
systèmes d’exploitation.

\sphinxAtStartPar
Pour installer Anaconda sur votre ordinateur, vous allez vous rendre sur
le \sphinxhref{http://docs.anaconda.com/anaconda/navigator/}{site officiel}%
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxnolinkurl{http://docs.anaconda.com/anaconda/navigator/}
%
\end{footnote}
depuis lequel l’on va télécharger directement la dernière version
d’Anaconda. Prenez la version du binaire qu’il vous faut :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Choisissez le système d’exploitation cible (Linux, Windows, Mac,
etc…)

\item {} 
\sphinxAtStartPar
Sélectionnez la version 3.X (à l’heure de l’écriture de ce document,
c’est la version 3.8 qui est proposée, surtout pensez à toujours
installer la version la plus récente de Python), compatible (64 bits
ou 32 bits) avec l’architecture de votre ordinateur.

\end{itemize}

\sphinxAtStartPar
Après le téléchargement, si vous êtes sur Windows, alors rien de bien
compliqué double cliquez sur le fichier exécutable et suivez les
instructions classique d’installation d’un logiciel sur Windows.

\sphinxAtStartPar
Si par contre vous êtes sur Linux, alors suivez les instructions qui
suivent:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ouvrez votre terminal et rassurez vous que votre chemin accès est
celui dans lequel se trouve votre fichier d’installation.

\item {} 
\sphinxAtStartPar
Exécutez la commande: \$ bash Anaconda3\sphinxhyphen{}2020.02\sphinxhyphen{}Linux\sphinxhyphen{}x86\_64.sh,
rassurez vous du nom du fichier d’installation, il peut changer selon
la version que vous choisissez.

\end{itemize}

\sphinxAtStartPar
Après que l’installation se soit déroulée normalement, éditez le fichier
caché \sphinxstylestrong{.bashrc} pour ajouter le chemin d’accès à Anaconda. Pour cela
exécutez les commandes suivantes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\$ cd \textasciitilde{}

\item {} 
\sphinxAtStartPar
\$ gedit .bashrc

\item {} 
\sphinxAtStartPar
Ajoutez cette commande à la dernière ligne du fichier que vous venez
d’ouvrir

\item {} 
\sphinxAtStartPar
export PATH= \textasciitilde{}/anaconda3/bin:\$PATH

\end{itemize}

\sphinxAtStartPar
Maintenant que c’est fait, enregistrez le fichier et fermez\sphinxhyphen{}le. Puis
exécutez les commandes suivantes
\begin{itemize}
\item {} 
\sphinxAtStartPar
\$ conda init

\item {} 
\sphinxAtStartPar
\$ Python

\end{itemize}

\sphinxAtStartPar
Pour ce qui est de l’installation sur Mac, veuillez suivre la procédure
d’installation dans la \sphinxhref{https://docs.anaconda.com/anaconda/install/mac-os/}{documentation
d’Anaconda}%
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.anaconda.com/anaconda/install/mac-os/}
%
\end{footnote}.

\sphinxAtStartPar
Il existe une distribution appelée
\sphinxhref{https://docs.conda.io/en/latest/miniconda.html}{Miniconda}%
\begin{footnote}[5]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.conda.io/en/latest/miniconda.html}
%
\end{footnote} qui est
un programme d’installation minimal gratuit pour conda. Il s’agit d’une
petite version bootstrap d’Anaconda qui inclut uniquement conda, Python,
les packages dont ils dépendent, et un petit nombre d’autres packages
utiles.

\sphinxAtStartPar
Terminons cette partie en nous familiarisant avec quelques notions de la
programmation Python.

\sphinxAtStartPar
\sphinxstylestrong{Première utilisation de Anaconda}

\sphinxAtStartPar
La distribution Anaconda propose deux moyens d’accéder à ses fonctions:
soit de manière graphique avec Anaconda\sphinxhyphen{}Navigator, soit en ligne de
commande (depuis Anaconda Prompt sur Windows, ou un terminal pour Linux
ou MacOS). Sous Windows ou MacOs, démarrez Anaconda\sphinxhyphen{}Navigator dans le
menu des programmes. Sous Linux, dans un terminal, tapez la commande : \$
anaconda\sphinxhyphen{}navigator (cette commande est aussi disponible dans le prompt
de Windows). Anaconda\sphinxhyphen{}Navigator propose différents services (déjà
installés, ou à installer). Son onglet Home permet de lancer le service
désiré. Les principaux services à utiliser pour développer des
programmes Python sont :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Spyder

\item {} 
\sphinxAtStartPar
IDE Python

\item {} 
\sphinxAtStartPar
Jupyter notebook et jupyter lab : permet de panacher des cellules de
commandes Python (code) et des cellules de texte (Markdown).

\end{itemize}

\sphinxAtStartPar
Pour la prise en main de Python nous allons utiliser jupyter lab.


\subsubsection{Prise en main de Python}
\label{\detokenize{chapter2:prise-en-main-de-python}}
\sphinxAtStartPar
Nous avons préparé un notebook qui nous permettra d’aller de zèro à demi
Héros en Python. Le notebook se trouve
\sphinxhref{https://colab.research.google.com/drive/1zILtNrCmPDFyQQ1Ev1H4jeHx7FuyEZ27?usp=sharing}{ici}%
\begin{footnote}[6]\sphinxAtStartFootnote
\sphinxnolinkurl{https://colab.research.google.com/drive/1zILtNrCmPDFyQQ1Ev1H4jeHx7FuyEZ27?usp=sharing}
%
\end{footnote}.


\paragraph{Python pour les debutants : Notions de bases du python}
\label{\detokenize{chapter2:python-pour-les-debutants-notions-de-bases-du-python}}

\subparagraph{Les nombres avec Python!}
\label{\detokenize{chapter2:les-nombres-avec-python}}
\sphinxAtStartPar
Dans cette partie, nous allons tout apprendre comment utiliser les
nombres sur Python.

\sphinxAtStartPar
Nous allons couvrir les points suivants :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Les types de nombres sur Python

\item {} 
\sphinxAtStartPar
Arithmetique de base sur les nombres

\item {} 
\sphinxAtStartPar
Différences entre Python 2 et 3

\item {} 
\sphinxAtStartPar
Assignation d’objets dans Python

\end{enumerate}


\subparagraph{Les types de nombres}
\label{\detokenize{chapter2:les-types-de-nombres}}
\sphinxAtStartPar
Python a plusieurs “types” de nombres. Nous nous occuperons
principalement des entiers (integers) et des nombres à virgule flottante
(float).

\sphinxAtStartPar
Le type entier (integer) représente des nombres entiers, positifs ou
negatifs. Par exemple: 7 et \sphinxhyphen{}1 sont des integers.

\sphinxAtStartPar
Le type flottant (float) de Python est facile à identifier parce que les
nombres sont représentés avec la partie décimal (Attention, ici les
nombre decimaux sont representer avec un point et non la virgule comme
habituellement en langue française), ou alors avec le signe exponentiels
(reprensenter par \sphinxstylestrong{e}) pour représenter les puissances de 10. Par
exemple, 2.0 et \sphinxhyphen{}2.1 sont des nombres de type flottant. 2e3 ( 2 fois 10
à la puissance 3) est aussi un nombre de type flottant en Python.

\sphinxAtStartPar
Voici un tableau des deux types que nous manipulerons le plus dans ce
cours:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Les types de nomber en Python.}\label{\detokenize{chapter2:id1}}\label{\detokenize{chapter2:table-nombre}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Exemples
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Type
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1, 4, \sphinxhyphen{}2, 100
&
\sphinxAtStartPar
Integers
\\
\sphinxhline
\sphinxAtStartPar
1.4, 0.3, \sphinxhyphen{}0.5, 2e2, 3e2
&
\sphinxAtStartPar
Floating\sphinxhyphen{}point numbers (float)
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Voyons maintenant, quelques examples d’arithmétique simples que l’on
peux appliquer sur ses elements.

\sphinxAtStartPar
Arithmétique

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Addition de 1 et 3 = 4}
\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mi}{3}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{4}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Soustraction de 1 dans 3 = 2}
\PYG{l+m+mi}{3}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Multiplication de 2 par 2 = 4}
\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{4}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Division de 5 par 2 = 2.5 (remaquez qu\PYGZsq{}on a la un flottant)}
\PYG{l+m+mi}{5}\PYG{o}{/}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{2.5}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Puissances. 2 a la  puissances 3 = 8}
\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{8}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}la racine carree peut se calculer  de la même manière = 2.0}
\PYG{l+m+mi}{4}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mf}{0.5}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{2.0}
\end{sphinxVerbatim}


\subparagraph{Ordre de priorite}
\label{\detokenize{chapter2:ordre-de-priorite}}
\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Voyons quel ordre de priorite python fais sur les operteurs}
\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{10} \PYG{o}{*} \PYG{l+m+mi}{10} \PYG{o}{+} \PYG{l+m+mi}{3}  \PYG{c+c1}{\PYGZsh{} avec ceci nous obtenons un resultats errone = 105}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{105}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Avec des parenthèse nous pouvons garder le contrôle de ces priorités}
\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{10}\PYG{o}{+}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{156}
\end{sphinxVerbatim}


\subparagraph{Les variables}
\label{\detokenize{chapter2:les-variables}}
\sphinxAtStartPar
Maintenant que nous savons comment utiliser python en mode calculette,
nous pouvons créer des variables et leur assigner des valeurs comme des
nombres. Notons que l’utilisation des variables sur python est un tout
petit peu different que ce qui ce passe dans d’autre langage de
programmation. Contrairement a d’autre language, python utilise une
technique de typage faible. Les variables ne sont pas declarees a
l’avance avant l’untilisation.

\sphinxAtStartPar
Il suffit d’un simple signe égal = pour assigner un nom à une variable.
Voyons quelques exemples pour détailler la façon de faire.

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Créons un objet nommé \PYGZdq{}x\PYGZdq{} et \PYGZdq{}y\PYGZdq{} et nous leur assignons la valeur 5 et .2 respectivement}
\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{5} \PYG{c+c1}{\PYGZsh{} ici la variable x est de type integer}
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mf}{.2} \PYG{c+c1}{\PYGZsh{} ici la variable y est de type flottant (notons que 0.2 peut aussi s\PYGZsq{}ecrire .2 tout simplement)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Maintenant, nous pouvons appeler \sphinxstyleemphasis{x} or \sphinxstyleemphasis{y} dans un script Python qui
les traitera comme le nombre 5 ou .2 respectivement

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} additionner des objets}
\PYG{n}{x}\PYG{o}{+}\PYG{n}{y}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{5.2}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Ré\PYGZhy{}assignation}
\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{10}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Vérification}
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{10}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Nous utilisons x pour assigner x de nouveau}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{x} \PYG{o}{+} \PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Vérification}
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{20}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Incrementation par 1 (on peux aussi incrementer avec n\PYGZsq{}importe quel nombre)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{+}\PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{21}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Autre syntaxe pour incrementer une variable}
\PYG{n}{x} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{22}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Decrementation par 1}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{21}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Autre syntaxe pour decrementer une variable}
\PYG{n}{x} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{20}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Nous pouvons aussi multiplier l\PYGZsq{}ancienne valeur de x par un quelconque nombre.}
\PYG{n}{x} \PYG{o}{*}\PYG{o}{=}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{40}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Nous pouvons aussi diviser l\PYGZsq{}ancienne valeur de x par un quelconque nombre.}
\PYG{n}{x} \PYG{o}{/}\PYG{o}{=}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{20.0}
\end{sphinxVerbatim}

\sphinxAtStartPar
NB: Toutes les operations que nous venons de voir s’applique aussi sur
les flottants

\sphinxAtStartPar
\sphinxstylestrong{Tout de meme, voici quelques règles à respecter pour créer un nom de
variable :}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Les noms ne doivent pas commencer par un nombre.

\item {} 
\sphinxAtStartPar
Pas d’espace dans les noms, utilisez \_ à la place

\item {} 
\sphinxAtStartPar
Interdit d’utiliser les symboles suivants : ’“,<>/?|()!@\#\$\%\textasciicircum{}\&*\textasciitilde{}\sphinxhyphen{}+

\item {} 
\sphinxAtStartPar
C’est une bonne pratique (PEP8) de mettre les noms en minuscules

\end{enumerate}

\sphinxAtStartPar
Les noms de variables permettent de conserver et manipuler plusieurs
valeurs de façon simple avec Python.

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Utilisez toujours des noms explicites pour mieux suivre ce que fait votre code !}
\PYG{n}{prix\PYGZus{}vente} \PYG{o}{=} \PYG{l+m+mi}{280}

\PYG{n}{prix\PYGZus{}unitaire} \PYG{o}{=} \PYG{l+m+mi}{150}

\PYG{n}{benefice} \PYG{o}{=} \PYG{n}{prix\PYGZus{}vente} \PYG{o}{\PYGZhy{}} \PYG{n}{prix\PYGZus{}unitaire}
\end{sphinxVerbatim}

\diilbookstyleinputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Visualiser le benefice !}
\PYG{n}{benefice}
\end{sphinxVerbatim}

\diilbookstyleoutputcell

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{130}
\end{sphinxVerbatim}


\subsection{Les Bases Mathématiques pour l’Apprentissage Automatique}
\label{\detokenize{chapter2:les-bases-mathematiques-pour-lapprentissage-automatique}}
\sphinxAtStartPar
Dans cette section, nous allons présenter les notions mathématiques
essentielles à l’apprentissage automatique (machine learning). Nous
n’aborderons pas les théories complexes des mathématiques afin de
permettre aux débutants (en mathématiques) ou mêmes les personnes hors
du domaine mais intéressées à l’apprentissage automatique de pouvoir en
profiter.


\subsubsection{Algèbre linéaire et Analyse}
\label{\detokenize{chapter2:algebre-lineaire-et-analyse}}
\sphinxAtStartPar
\sphinxstylestrong{Définition d’espaces vectoriels.} Un espace vectoriel est un triplet
\((V, +, *)\) formé d’un ensemble \(V\) muni de deux lois,
\begin{equation}\label{equation:chapter2:chapter2:0}
\begin{split}\begin{aligned}
     +:\quad & V\times V \longrightarrow{V}\\
    &(u,v)\mapsto u+v \\
    \text{et} \qquad \qquad\\
    *:\quad &\mathbb{K}\times V \longrightarrow{V}, \text{ avec $\mathbb{K}$ un corps commutatif}\\
    &(\lambda,v)\mapsto \lambda * v=\lambda v\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
qui vérifient:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\text{ associativité de } + :  \forall\  u,v, w \in V, \quad (u+v)+w=u+(v+w)\)

\item {} 
\sphinxAtStartPar
\(\text{ commutativité de } + : \forall\ u,v\in V,\quad u+v=v+u\)

\item {} 
\sphinxAtStartPar
\(\text{ existence d'élément neutre pour } + :  \exists~ e \in V : \forall\ u \in V, \quad u+e=e+u=u\)

\item {} 
\sphinxAtStartPar
\(\text{ existence d'élément opposé pour } + : \forall \ u \in V, \exists ~ v \in V :u+v=v+u=0. \text{ On note } v=-u \text{ et } v \text{ est appelé l'opposé de } u\)

\item {} 
\sphinxAtStartPar
\(\text{ existence de l'unité pour } * :  \exists~ e \in \mathbb{K} \text{ tel que } \forall\  u\in V,\quad e*u=u\)

\item {} 
\sphinxAtStartPar
\(\text{ associativité de } * :  \forall\  (\lambda_1, \lambda_2, u) \in \mathbb{K} \times \mathbb{K}\times V,\quad  (\lambda_1 \lambda_2)* u =\lambda_1*(\lambda_2 * u)\)

\item {} 
\sphinxAtStartPar
\(\text{ somme de vecteurs (distributivité de * sur +)} :  \forall\  (\lambda, u, v) \in \mathbb{K}\times V\times V, \quad\lambda*(u+v)=\lambda * u+\lambda * v\)

\item {} 
\sphinxAtStartPar
:
\(\forall\  (\lambda_1, \lambda_2, u) \in \mathbb{K} \times \mathbb{K}\times V,\quad  (\lambda_1+\lambda_2) * u=\lambda_1 * u +\lambda_2 * u.\)

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Remarque 1:} Les éléments de \(V\) sont appelés des
\sphinxstylestrong{vecteurs}, ceux de \(\mathbb{K}\) sont appelés des
\sphinxstylestrong{scalaires} et l’élément neutre pour \(+\) est appelé \sphinxstylestrong{vecteur
nul}. Finalement, \(V\) est appelé \(\mathbb{K}\)\sphinxhyphen{}espace
vectoriel ou espace vectoriel sur \(\mathbb{K}\).
\item[] \sphinxstylestrong{Base d’un espace vectoriel.} Soit \(V\) un
\(\mathbb{K}\)\sphinxhyphen{}espace vectoriel. Une famille de vecteurs
\item[] \(\mathcal{B}=\big\{b_1, b_2, \dots, b_n\big\}\) est appelée base
de \(V\) si les deux propriétés suivantes sont satisfaites:
\end{DUlineblock}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\forall\  u \in V, \exists~ c_1, \dots, c_n \in \mathbb{K}\)
tels que \(\displaystyle u = \sum_{i=1}^{n}c_i b_i\)
(On dit que \(\mathcal{B}\) est
\(\textbf{une famille génératrice}\) de \(V\)).

\item {} 
\sphinxAtStartPar
\(\displaystyle \forall\  \lambda_1, \dots, \lambda_n \in \mathbb{K}, \quad\sum_{i=1}^{n}\lambda_i b_i=0\Longrightarrow \lambda_i = 0 \quad\forall\  i\).
(On dit que les éléments de \(\mathcal{B}\) sont \sphinxstyleemphasis{linéairement
indépendants}).

\end{itemize}

\begin{DUlineblock}{0em}
\item[] Lorsque \(\displaystyle u = \sum_{i=1}^{n}c_i b_i\), on dit que
\(c_1, \dots, c_n\) sont les coordonnées de \(u\) dans la base
\(\mathcal{B}\). Si de plus aucune confusion n’est à craindre, on
peut écrire:
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:1}
\begin{split}\mathbf{u}=\begin{bmatrix}
c_1\\c_2\\ \vdots\\ c_n
\end{bmatrix}.\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstylestrong{Définition.} Le nombre d’éléments dans une base d’un espace
vectoriel est appelé \sphinxstylestrong{dimension} de l’espace vectoriel.
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{NB:} Un espace vectoriel ne peut être vide (il contient toujours le
vecteur nul). L’\sphinxstylestrong{espace vectoriel nul} \(\{0\}\) n’a pas de
base et est \sphinxstylestrong{de dimension nulle}. Tout \sphinxstylestrong{espace vectoriel non nul}
de dimension finie admet une infinité de bases mais sa \sphinxstylestrong{dimension est
unique}.
\item[] \sphinxstylestrong{Exemples d’espaces vectoriels}: Pour tous \(n,m\ge1\),
l’ensemble des matrices \(\mathcal{M}_{nm}\) à coefficients réels
et l’ensemble \(\mathbb{R}^n\) sont des \(\mathbb{R}\)\sphinxhyphen{}espace
vectoriels. En effet, il est très facile de vérifier que nos exemples
satisfont les huit propriétés énoncées plus haut. Dans le cas
particulier \(V=\mathbb{R}^n\), toute famille d’exactement
\(n\) vecteurs linéairement indépendants en est une base. En
revanche, toute famille de moins de \(n\) vecteurs ou qui contient
plus que \(n\) vecteurs ne peut être une base de
\(\mathbb{R}^n\).
\end{DUlineblock}

\sphinxAtStartPar
\sphinxstylestrong{Matrices}: Soit \(\mathbb{K}\) un corps commutatif. Une matrice
en mathématiques à valeurs dans \(\mathbb{K}\) est un tableau de
nombres, où chaque nombre est un élément de \(\mathbb{K}\). Chaque
ligne d’une telle matrice est un vecteur (élément d’un
\(\mathbb{K}\)\sphinxhyphen{}espace vectoriel). Une matrice est de la forme:
\begin{equation}\label{equation:chapter2:chapter2:2}
\begin{split}\mathbf{M}=\begin{bmatrix}
a_{11} &a_{12} \dots a_{1n}\\
a_{21} &a_{22} \dots a_{2n}\\
\vdots\\
a_{m1}& a_{m2} \dots a_{mn}
\end{bmatrix}.\end{split}
\end{equation}
\sphinxAtStartPar
On note aussi
\(\mathbf{M}=\big{(}a_{ij}\big{)}_{1\le i\le m, 1\le j\le n}\).

\begin{DUlineblock}{0em}
\item[] La matrice ci\sphinxhyphen{}dessus est carrée si \(m=n\). Dans ce cas, la suite
\([a_{11}, a_{22}, \dots, a_{mm}]\) est appelée \sphinxstylestrong{diagonale} de
\(M\). Si tous les coefficients hors de la diagonale sont zéro, on
dit que la matrice est diagonale. Une matrice avec tous ses
coefficients nuls est dite matrice \sphinxstylestrong{nulle}.
\item[] \sphinxstylestrong{Produit de matrices.} Soient
\(\mathbf{A}=\big{(}a_{ij}\big{)}_{1\le i\le m, 1\le j\le n}, \mathbf{B}=\big{(}b_{ij}\big{)}_{1\le i\le n, 1\le j\le q}\)
deux matrices.
\item[] On définit le produit de \(\mathbf{A}\) par \(\mathbf{B}\) et
on note \(\mathbf{A}\times \mathbf{B}\) ou simplement
\(\mathbf{A}\mathbf{B}\), la matrice \(M\) définie par:
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:3}
\begin{split}\begin{aligned}
    \mathbf{M}_{ij} = \sum_{\ell=1}^{n}a_{i\ell}b_{\ell j}, \text{ pour tout } i \text{ et } j.\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstylestrong{Important.}
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Le produit \(\mathbf{AB}\) est possible si et seulement si le
nombre de colonnes de \(\mathbf{A}\) est égal au nombre de lignes
de \(\mathbf{B}\).

\item {} 
\sphinxAtStartPar
Dans ce cas, \(\mathbf{AB}\) a le même nombre de lignes que
\(\mathbf{A}\) et le même nombre de colonnes que
\(\mathbf{B}\).

\item {} 
\sphinxAtStartPar
Un autre point important à noter est que le produit matriciel n’est
pas commutatif (\(\mathbf{AB}\) n’est pas toujours égal à
\(\mathbf{BA}\)).

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Exemple.} Soient les matrices \(\mathbf{A}\) et
\(\mathbf{B}\) définies par:
\begin{equation}\label{equation:chapter2:chapter2:4}
\begin{split}\mathbf{A} = \begin{bmatrix}
2 & -3 & 0\\
5 &11 & 5\\
1& 2 & 3
\end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix}
1 & 3\\
-5 &1\\
1 & 2
\end{bmatrix},
\mathbf{A}+\mathbf{B} = \begin{bmatrix}
1 & 3\\
-5 &1\\
1 & 2
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
Le nombre de colonnes de la matrice \(\mathbf{A}\) est égal au
nombre de lignes de la matrice \(\mathbf{B}\).
\begin{equation}\label{equation:chapter2:chapter2:5}
\begin{split}\mathbf{AB} = \begin{bmatrix}
2\times1+(-3)\times(-5)+0\times1 & 2\times3+(-3)\times1+0\times2\\
5\times1+11\times(-5)+5\times1 &5\times3+11\times1+5\times2\\
1\times1+2\times(-5)+3\times1& 1\times3+2\times1+3\times2
\end{bmatrix} = \begin{bmatrix}
17 & 3\\
-45 &33\\
-6 & 11
\end{bmatrix}.\end{split}
\end{equation}
\sphinxAtStartPar
Le produit \(\mathbf{BA}\) n’est cependant pas possible.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Somme de matrices et multiplication d’une matrice par un scalaire.}
\item[] La somme de matrices et multiplication d’une matrice par un scalaire
se font coefficients par coefficients.
\item[] Avec les matrice \(\mathbf{A}, \mathbf{B}\) de l’exemple
précédent, et
\(\mathbf{C}=\begin{bmatrix} -2 & -7 & 3\\ 5 &10 & 5\\ 12& 9 & 3 \end{bmatrix}\),
on a:
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:6}
\begin{split}\mathbf{A}+ \mathbf{C} = \begin{bmatrix}
2+(-2) & -3+(-7) & 0+3\\
5+5 &11+10 & 5+5\\
1+12& 2+9 & 3+3
\end{bmatrix}=\begin{bmatrix}
0 & -10 & 3\\
10 &21 & 10\\
13& 11 & 6
\end{bmatrix}, \text{ et pour tout } \lambda \in \mathbb{R}, \quad
\lambda \mathbf{B} = \begin{bmatrix}
\lambda & 3 \lambda\\
-5 \lambda & \lambda\\
\lambda & 2\lambda
\end{bmatrix}.\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{NB:} La somme de matrice n’est définie que pour des matrices de
même taille.
\item[] \sphinxstylestrong{Déterminant d’une matrice.}
\end{DUlineblock}

\sphinxAtStartPar
Soit \(\mathbf{A}=(a_{ij})_{1\le i\le n, 1\le j\le n}\) une matrice
carrée d’ordre \(n\). Soit \(\mathbf{A}_{i,j}\) la sous\sphinxhyphen{}matrice
de \(\mathbf{A}\) obtenue en supprimant la ligne \(i\) et la
colonne \(j\) de \(\mathbf{A}\). On appelle \sphinxstylestrong{déterminant} (au
développement suivant la ligne \(i\)) de \(\mathbf{A}\) et on
note \(\operatorname{det}(\mathbf{A})\), le nombre
\begin{equation}\label{equation:chapter2:chapter2:7}
\begin{split}\begin{aligned}
    \operatorname{det}(\mathbf{A}) = \sum_{j=1}^{n}a_{ij}(-1)^{i+j}\operatorname{det}(\mathbf{A}_{i,j}),\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
avec le déterminant d’une matrice carrée de taille \(2\times 2\)
donné par:
\begin{equation}\label{equation:chapter2:chapter2:8}
\begin{split}\begin{aligned}
    \operatorname{det} \left(\begin{bmatrix}
a & b\\
c & d\\
\end{bmatrix}\right) = ad-bc.\end{aligned}\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{NB:} Le développement suivant toutes les lignes donne le même
résultat.
\item[] Le déterminant d’une matrice a une deuxième formulation dite de
\sphinxhref{https://fr.wikipedia.org/wiki/Formule\_de\_Leibniz\#D}{Leibniz}%
\begin{footnote}[7]\sphinxAtStartFootnote
\sphinxnolinkurl{https://fr.wikipedia.org/wiki/Formule\_de\_Leibniz\#D}
%
\end{footnote} que
nous n’introduisons pas dans ce document.
\item[] \sphinxstylestrong{Inverse d’une matrice.} Soit \(\mathbf{A}\) une matrice carrée
d’ordre \(n\). \(\mathbf{A}\) est \sphinxstylestrong{inversible} s’il existe
une autre matrice notée \(\mathbf{A}^{-1}\) telle que
\(\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}_n\),
où \(\mathbf{I}_n\) est la matrice identité de taille
\(n\times n\).
\item[] Les matrices, leurs inverses et les opérations sur les matrices sont
d’une importance capitale dans l’apprentissage automatique.
\item[] \sphinxstylestrong{Vecteurs propres, valeurs propres d’une matrice.}
\item[] Soient \(E\) un espace vectoriel et \(\mathbf{A}\) une
matrice. Un vecteur \(\mathbf{v}\in E\) est dit \sphinxstylestrong{vecteur propre}
de \(\mathbf{A}\) si \(\mathbf{v}\neq 0\) et il existe un
scalaire \(\lambda\) tel que
\(\mathbf{A}\mathbf{v}=\lambda \mathbf{v}\). Dans ce cas, on dit
que \(\lambda\) est la \sphinxstylestrong{valeur propre} associée au vecteur
propre \(\mathbf{v}\).
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Applications linéaires et changement de base d’espaces vectoriels.}
\item[] Soient \((E, \mathcal{B}),\ (F, \mathcal{G})\) deux
\(\mathbb{K}\)\sphinxhyphen{}espace vectoriels, chacun muni d’une base et
\(f:\ E \rightarrow F\) une application.
\item[] On dit que \(f\) est \sphinxstylestrong{linéaire} si les propriétés suivantes sont
satisfaites:
\end{DUlineblock}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Pour tous \(\mathbf{u}, \mathbf{v}\in E\),
\(f(\mathbf{u}+\mathbf{v})=f(\mathbf{u})+f(\mathbf{v})\).

\item {} 
\sphinxAtStartPar
Pour tout \((\lambda, \mathbf{u}) \in \mathbb{K}\times E\),
\(f(\lambda \mathbf{u})=\lambda f(\mathbf{u})\).

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] On suppose que \(\mathcal{B}=\{e_1, e_2, \dots, e_n\}\) et
\(\mathcal{G}=\{e'_1, e'_2, \dots, e'_m\}\).
\item[] De manière équivalente, \(f\) est linéaire s’il existe une matrice
\(\mathbf{A}\) telle que pour tout
\(\mathbf{x}\in E, \quad f(x)=\mathbf{A}\mathbf{x}\).
\item[] Dans ce cas, la matrice \(\mathbf{A}\) que l’on note
\(Mat_{\mathcal{B},\mathcal{G}}(f)\) est appelée matrice
(représentative) de l’application linéaire \(f\) dans le couple de
coordonnées \((\mathcal{B},\mathcal{G})\).
\item[] La matrice \(\mathbf{A}\) est unique et de taille
\(m\times n\) (notez la permutation \sphinxstyleemphasis{dimension de l’espace
d’arrivée puis dimension de l’espace de départ dans la taille de la
matrice}). De plus, la colonne \(j\) de la matrice
\(\mathbf{A}\) est constituée des coordonnées de \(f(e_j)\)
dans la base \(\mathcal{G}\) de \(F\). Lorsque \(E=F\),
l’application linéaire \(f\) est appelée \sphinxstylestrong{endomorphisme} de
\(E\) et on écrit simplement \(Mat_{\mathcal{B}}(f)\) au lieu
de \(Mat_{\mathcal{B},\mathcal{G}}(f)\).
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.} Soient \(E\) un espace vectoriel de dimension
finie et, \(\mathcal{B}\) et \(\mathcal{C}\), deux bases de
\(E\). On appelle \sphinxstylestrong{matrice de passage} de la base
\(\mathcal{B}\) à la base \(\mathcal{C}\) la matrice de
l’application identité
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:9}
\begin{split}\begin{aligned}
    id_E: \quad &(E, \mathcal{C})\rightarrow (E, \mathcal{B}):\\
    & x \mapsto x\quad .\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Cette matrice est notée \(P_{\mathcal{B}}^{\mathcal{C}}\) et on
a
\(P_{\mathcal{B}}^{\mathcal{C}}:=Mat_{\mathcal{C},\mathcal{B}}(id_E)\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Note:} Si
\(\mathbf{x}=\begin{bmatrix} x_1\\x_2\\ \vdots\\ x_n \end{bmatrix}\)
est un vecteur de \(E\) exprimé dans la base \(\mathcal{B}\),
alors l’expression de \(\mathbf{x}\) dans la base
\(\mathcal{C}\) est donnée par
\(\begin{bmatrix} x'_1\\x'_2\\ \vdots\\ x'_n \end{bmatrix}=(P_{\mathcal{B}}^{\mathcal{C}})^{-1}\mathbf{x}=P_{\mathcal{C}}^{\mathcal{B}}\mathbf{x}\).
\item[] \sphinxstylestrong{Exemple.} Si \(E=\mathbb{R}^3\) avec ses deux bases
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:10}
\begin{split}\mathcal{B}=\left(
\begin{bmatrix}
1\\0\\0
\end{bmatrix},\begin{bmatrix}
0\\1\\0
\end{bmatrix},\begin{bmatrix}
0\\0\\1
\end{bmatrix}
\right) \text{ et } \mathcal{C}=\left(
\begin{bmatrix}
-1\\2\\3
\end{bmatrix},\begin{bmatrix}
0\\1\\5
\end{bmatrix},\begin{bmatrix}
0\\0\\1
\end{bmatrix}
\right),\end{split}
\end{equation}
\sphinxAtStartPar
on a
\(P_{\mathcal{B}}^{\mathcal{C}} = \begin{bmatrix} -1&0&0\\ 2&1&0\\ 3&5&1 \end{bmatrix}\)
(c’est\sphinxhyphen{}à\sphinxhyphen{}dire qu’on exprime les vecteurs de \(\mathcal{C}\) dans
\(\mathcal{B}\) pour former
\(P_{\mathcal{B}}^{\mathcal{C}}\)).
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Formule du changement de base pour une application linéaire.}
\item[] Soient \(E\) une application linéaire et, \(\mathcal{B}\) et
\(\mathcal{C}\), deux bases de \(E\). Alors
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:11}
\begin{split}Mat_{\mathcal{C}}(f)=P_{\mathcal{C}}^{\mathcal{B}} Mat_{\mathcal{B}}(f)P_{\mathcal{B}}^{\mathcal{C}},\end{split}
\end{equation}
\sphinxAtStartPar
ou encore
\begin{equation}\label{equation:chapter2:chapter2:12}
\begin{split}Mat_{\mathcal{C}}(f)=(P_{\mathcal{B}}^{\mathcal{C}})^{-1} Mat_{\mathcal{B}}(f)P_{\mathcal{B}}^{\mathcal{C}}.\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Diagonalisation et décomposition en valeurs singulières.}
\item[] \sphinxstylestrong{Diagonalisation.} Soit \(\mathbf{A}\) une matrice carrée à
coefficients dans
\(\mathbb{K}=\mathbb{R} \text{ ou } \mathbb{C}\). On dit que
\(\mathbf{A}\) est \sphinxstylestrong{diagonalisable} s’il existe une matrice
inversible \(\mathbf{P}\) et une matrice diagonale
\(\mathbf{D}\) telles que
\(\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}\). On dit aussi
que \(\mathbf{A}\) est similaire à \(\mathbf{D}\).
\item[] \sphinxstylestrong{Important.} Soient \(E\) un espace vectoriel de dimension finie
et \(f\) un endomorphisme de \(E\) de matrice représentative
(dans une base \(\mathcal{B}\) de \(E\)) diagonalisable
\(\mathbf{A}=\mathbf{P}\mathbf{D}\mathbf{P}^{-1}\). On rappelle
que les colonnes de \(\mathbf{P}\) sont les vecteurs propres de
\(\mathbf{A}\). Alors ces colonnes (dans leur ordre) constituent
une base de \(E\), et dans cette base, la matrice
\(\mathbf{A}\) est représentée par la matrice diagonale
\(\mathbf{D}\). En d’autres termes, si \(\mathcal{C}\) est la
base des vecteurs propres de \(\mathbf{A}\), alors
\(Mat_{\mathcal{C}}(f)=\mathbf{D}\). Enfin, la matrice
\(\mathbf{D}\) est constituée des valeurs propres de
\(\mathbf{A}\) et le processus de calcul de \(\mathbf{P}\) et
\(\mathbf{D}\) est appelé \sphinxstylestrong{diagonalisation}.
\item[] \sphinxstylestrong{Décomposition en valeurs singulières.}
\item[] Soit \(\mathbf{M}\) une matrice de taille \(m\times n\) et à
coefficients dans
\(\mathbb{K}=\mathbb{R} \text{ ou } \mathbb{C}\). Alors
\(\mathbf{M}\) admet une factorisation de la forme
\(\mathbf{M}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*\), où \$\$
\end{DUlineblock}
\begin{itemize}
\item {} 
\begin{DUlineblock}{0em}
\item[] \(\mathbf{U}\) est une matrice unitaire (sur
\(\mathbb{K}\)) de taille \(m\times m\).
\end{DUlineblock}

\item {} 
\begin{DUlineblock}{0em}
\item[] \(\mathbf{V}^*\) est l’adjoint (conjugué de la transposée) de
\(\mathbf{V}\), matrice unitaire (sur \(\mathbb{K}\)) de
taille \(n\times n\)
\end{DUlineblock}

\item {} 
\sphinxAtStartPar
\(\mathbf{\Sigma}\) est une matrice de taille \(m\times n\)
dont les coefficients diagonaux sont les valeurs singulières de
\(\mathbf{M}\), i.e, les racines carrées des valeurs propres de
\(\mathbf{M}^*\mathbf{M}\) et tous les autres coefficients sont
nuls.

\end{itemize}

\sphinxAtStartPar
Cette factorisation est appelée \sphinxstylestrong{la décomposition en valeurs
singulières} de \(\mathbf{M}\). \sphinxstylestrong{Important.} Si la matrice
\(\mathbf{M}\) est de rang \(r\), alors
\begin{itemize}
\item {} 
\begin{DUlineblock}{0em}
\item[] les \(r\) premières colonnes de \(\mathbf{U}\) sont les
vecteurs singuliers à gauche de \(\mathbf{M}\)
\end{DUlineblock}

\item {} 
\sphinxAtStartPar
les \(r\) premières colonnes de \(\mathbf{V}\) sont les
vecteurs singuliers à droite de \(\mathbf{M}\)

\item {} 
\sphinxAtStartPar
les \(r\) premiers coefficients strictement positifs de la
diagonale de \(\mathbf{\Sigma}\) sont les valeurs singulières de
\(\mathbf{M}\) et tous les autres coefficients sont nuls.

\end{itemize}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Produit scalaire et normes vectorielles.} Soit \(V\) un espace
vectoriel sur \(\mathbb{R}\).
\item[] On appelle produit scalaire sur \(V\) toute application
\end{DUlineblock}
\begin{equation}\label{equation:chapter2:chapter2:13}
\begin{split}\begin{aligned}
    \big{<}.,.\big{>}:\quad &V\times V\rightarrow{\mathbb{R}}\\
    &(\mathbf{u},\mathbf{v})\mapsto \big{<}\mathbf{u},\mathbf{v}\big{>}, \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
telle que,
\(\forall ( \lambda_1, \lambda_2, \mathbf{u}, \mathbf{v}, \mathbf{w}) \in \mathbb{R}\times\mathbb{R}\times V\times V \times V,\)
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\langle\mathbf{u}, \mathbf{v}\rangle = \langle\mathbf{v}, \mathbf{u}\rangle\)(symétrie)

\item {} \begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\langle\lambda_1 \mathbf{u}+\lambda_2 \mathbf{v}, \mathbf{w}\rangle = \lambda_1\langle \mathbf{u},\mathbf{w}\rangle +\lambda_2\langle \mathbf{v},\mathbf{w}\rangle\)
(linéarité à gauche)

\item {} 
\sphinxAtStartPar
\(\langle\mathbf{u}, \lambda_1 \mathbf{v}+\lambda_2 \mathbf{w}\rangle = \lambda_1\langle \mathbf{u},\mathbf{v}\rangle +\lambda_2\langle \mathbf{u},\mathbf{w}\rangle\)
(linéarité à droite)

\end{enumerate}

\item {} 
\sphinxAtStartPar
\(\langle\mathbf{u}, \mathbf{u}\rangle \geq 0 \qquad\) (positive)

\item {} 
\sphinxAtStartPar
\(\langle\mathbf{u}, \mathbf{u}\rangle = 0 \implies \mathbf{u}=0 \qquad\)
(définie)

\end{itemize}
\begin{equation}\label{equation:chapter2:chapter2:14}
\begin{split}\begin{aligned}
    \|.\|: \quad&V\rightarrow{\mathbb{R_{+}}}\\
    &\mathbf{v}\mapsto \|\mathbf{v}\|\end{aligned}\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] \(V\)
\(\forall\  (\lambda, \mathbf{u}, \mathbf{v})\in \mathbb{R}\times V\times V\)
\end{DUlineblock}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\|\lambda\mathbf{u}\| = |\lambda| \times \|\mathbf{u}\|\)

\item {} 
\sphinxAtStartPar
\(\|\mathbf{u} + \mathbf{v} \| \leq \|\mathbf{u}\| + \|\mathbf{u}\| + \|v\|\)(inégalité
triangulaire)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Remarque 2} Si \(\big{<}.,.\big{>}\) est un produit scalaire sur
\(V\), alors \(\big{<}.,.\big{>}\) induit une norme sur
\(V\). En effet,

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:15}
\begin{split}\begin{aligned}
    \|.\|_{\big{<}.,.\big{>}}:\quad&V\rightarrow{\mathbb{R_{+}}}\\
    & \mathbf{u}\mapsto \|\mathbf{u}\|=\sqrt{\big{<}\mathbf{u},\mathbf{u}\big{>}}\end{aligned}\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Exemples de normes et produits scalaires.}
\item[] Prenons \(V=\mathbb{R}^n\).
\item[] \(\bullet\) Les applications
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:16}
\begin{split}\begin{aligned}
    \rho:\quad &V\times V\rightarrow{\mathbb{R}}\\
    &(\mathbf{u},\mathbf{v})\mapsto \sum_{i=1}^{n}u_iv_i ,\end{aligned}\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] et
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:17}
\begin{split}\begin{aligned}
    \mu:\quad &V\rightarrow{\mathbb{R}_+}\\
    &\mathbf{u}\mapsto \sqrt{\sum_{i=1}^{n}u_i^2},\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
sont respectivement un produit scalaire et une norme sur \(V\).
Il faut remarquer que
\(\forall\  \mathbf{u} \in V, \quad \mu(\mathbf{u})=\sqrt{\rho(\mathbf{u},\mathbf{u})}\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] \(\bullet\) Pour tout \(p\in \mathbb{N}^*\), l’application
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:18}
\begin{split}\begin{aligned}
    \mu_p:\quad &V\rightarrow{\mathbb{R}_+}\\
    &\mathbf{u}\mapsto \bigg{(}\sum_{i=1}^{n}|u_i|^p\bigg{)}^{\frac{1}{p}},\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
est une norme sur \(V\) appelée norme \(p\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] Dans le cas \(p=2\), on retrouve la norme \(\mu\) ci\sphinxhyphen{}dessus
appelée norme euclidienne.
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Remarque 3.} Un espace vectoriel muni d’une norme est appelé
\sphinxstylestrong{espace vectoriel normé}.
\item[] \sphinxstylestrong{Notion de distance.}
\item[] Soit \(E\) un ensemble non vide. Toute application
\(d:E \times E \rightarrow \mathbb{R}_{+}\) qui satisfait pour
tout \(x, y, z \in E\):
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:19}
\begin{split}\begin{aligned}
    &\bullet \quad d(x,y) = d(y,x) \text{ (symétrie)}\\
    &\bullet \quad d(x,y) = 0 \Longrightarrow x=y\text{ (séparation)}\\
    &\bullet \quad d(x,y) \le d(x,z)+d(z,y) \text{ (inégalité triangulaire)}\end{aligned}\end{split}
\end{equation}\end{quote}


\paragraph{Exemples de distances.}
\label{\detokenize{chapter2:exemples-de-distances}}
\sphinxAtStartPar
\(\bullet\)
\begin{equation}\label{equation:chapter2:chapter2:20}
\begin{split}\begin{aligned}
    d:\quad &\mathbb{R}^n\times \mathbb{R}^{n}\rightarrow{\mathbb{R}_+}\nonumber\\
    &(\mathbf{u}, \mathbf{v})\mapsto \bigg{(}\sum_{i=1}^{n}|u_i-v_i|\bigg{)}.
    \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
\(\bullet\)
\begin{equation}\label{equation:chapter2:chapter2:21}
\begin{split}\begin{aligned}
    d:\quad &\mathbb{R}^n\times \mathbb{R}^n\rightarrow{\mathbb{R}_+}\nonumber \\
    &(\mathbf{u}, \mathbf{v})\mapsto \bigg{(}\sum_{i=1}^{n}|u_i-v_i|^2\bigg{)}^{\frac{1}{2}}.
    \end{aligned}\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] \(\bullet\) C’est la généralisation de la distance euclidienne et
de la distance de Manhattan
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:22}
\begin{split}\begin{aligned}
    d_{Minkowski}:\quad &\mathbb{R}^n\times \mathbb{R}^n\rightarrow{\mathbb{R}_+}\nonumber \\
    &(\mathbf{u}, \mathbf{v})\mapsto \bigg{(}\sum_{i=1}^{n}|u_i-v_i|^p\bigg{)}^{\frac{1}{p}}, p\ge 1. \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstylestrong{Espaces métriques.}
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.} Un \sphinxstylestrong{espace métrique} est un ensemble \(E\) muni
d’une distance \(d\); on écrit \((E, d)\).
\item[] \sphinxstylestrong{Remarque 4.} Tout espace vectoriel normé est un espace métrique.
\item[] \sphinxstylestrong{Suites dans un espace métrique.}
\item[] Soit \((E,d)\) un espace métrique. On appelle \sphinxstylestrong{suite}
(d’éléments de \(E\)) et on note \((u_n)_{n\in I}\) ou
\((u)_n\) une application:
\end{DUlineblock}
\begin{equation}\label{equation:chapter2:chapter2:23}
\begin{split}\begin{aligned}
    u: \quad& I \rightarrow E\\
    & n \mapsto u(n):=u_n\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
où \(I\) est une partie infinie de \(\mathbb{N}\). On dit que
la suite \((u)_n\) converge vers \(u^*\in E\) si pour tout
\(\epsilon>0\) il existe \(N \in \mathbb{N}\) tels que:
\begin{equation}\label{equation:chapter2:chapter2:24}
\begin{split}\begin{aligned}
    \forall\  n\in \mathbb{N}, \quad n>N \Longrightarrow d(u_n, u^*) < \epsilon\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
En d’autres termes, la suite \((u)_n\) converge vers
\(u^*\in E\) si pour tout \(\epsilon>0\), il existe un entier
\(N\in \mathbb{N}\) tel que pour tout \(n>N\), \(u_n\) est
contenu dans la boule \(\mathcal{B}_{\epsilon}\) centrée en
\(u^*\) et de rayon \(\epsilon\).

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{NB:} La suite \((u)_n\) à valeurs dans \(E\) peut converger
dans un ensemble autre que \(E\).
\item[] \sphinxstylestrong{Définition.} La suite \((u)_n\) d’éléments de \(E\) est
dite de Cauchy si pour tout \(\epsilon>0\), il existe
\(N\in\mathbb{N}\) tel que:
\end{DUlineblock}
\begin{equation}\label{equation:chapter2:chapter2:25}
\begin{split}\begin{aligned}
    \forall\  n>m \in \mathbb{N},\quad m>N \Longrightarrow d(u_n, u_m)<\epsilon.\end{aligned}\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] Autrement dit, tous les termes \(u_n, u_m\) d’une suite de Cauchy
se rapprochent de plus en plus lorsque \(n\) et \(m\) sont
suffisamment grands.
\item[] \sphinxstylestrong{Espaces métriques complets.}
\item[] \sphinxstylestrong{Définition.} Un espace métrique \((E,d)\) est dit \sphinxstylestrong{complet}
si toute suite de Cauchy de \(E\) converge dans \(E\).
\item[] Un espace métrique complet est appelé \sphinxstylestrong{espace de Banach}.
\end{DUlineblock}


\subsubsection{Calcul du gradient (dérivation).}
\label{\detokenize{chapter2:calcul-du-gradient-derivation}}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Fonction réelle.}
\item[] \sphinxstylestrong{Définition.}
\item[] Soit \(f: J\rightarrow \mathbb{R}\) une fonction, avec \(J\)
un intervalle ouvert de \(\mathbb{R}\).
\item[] On dit que \(f\) est \sphinxstylestrong{dérivable} en \(a\in J\) si la limite:
\end{DUlineblock}
\begin{equation}\label{equation:chapter2:chapter2:26}
\begin{split}\begin{aligned}
    \lim_{h\rightarrow 0} \frac{f(a+h)-f(a)}{h} \text{ est finie.}
\end{aligned}\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] Si \(f\) est dérivable en \(a\), la dérivée de \(f\) en
\(a\) est notée \(f'(a).\) La fonction dérivée de \(f\)
est notée \(f'\) ou \(\frac{\mathrm{d}f}{\mathrm{d}x}\) ou
\(\mathrm{d}f\).
\item[] \sphinxstylestrong{Exemple de dérivées.}
\end{DUlineblock}
\begin{itemize}
\item {} 
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Fonctions polynomiales.}
\item[] La dérivée de la fonction
\(f(x)=a_nx^n+a_{n-1}x^{n-1}+\dots + a_1x+a_0\), avec les
\(a_i\) des constantes, est
\(f'(x)=na_nx^{n-1}+(n-1)a_{n-1}x^{n-2}+\dots+a_1\).
\end{DUlineblock}

\item {} 
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Fonction exponentielle de base :math:`e`}.
\item[] La dérivée de la fonction \(f(x)=\exp(x)\) est la fonction
\(f\) elle\sphinxhyphen{}même, i.e,
\(\frac{\mathrm{d}\exp}{\mathrm{d}x}(x)=\exp(x)\).
\end{DUlineblock}

\item {} 
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Fonctions trigonométriques.}
\item[] \(\frac{\mathrm{d}\cos}{\mathrm{d}x}( x)=-\sin x\) et
\(\frac{\mathrm{d} \sin}{dx}(x)=\cos x\).
\end{DUlineblock}

\item {} 
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Fonction logarithme népérien}.
\item[] \(\frac{\mathrm{d} \ln}{\mathrm{d}x} (x) = \frac{1}{x}\).
\end{DUlineblock}

\end{itemize}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Propriétés.}
\item[] Soient \(J\subseteq \mathbb{R}\) un intervalle ouvert,
\(u,\ v\ : J\rightarrow \mathbb{R}\) deux fonctions et
\(\lambda \in \mathbb{R}\). Alors on a les propriétés suivantes de
la dérivée:
\end{DUlineblock}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\begin{DUlineblock}{0em}
\item[] \((u+v)' = u'+v'\)
\end{DUlineblock}

\item {} 
\begin{DUlineblock}{0em}
\item[] \((uv)' = uv'+u'v\)
\end{DUlineblock}

\item {} 
\sphinxAtStartPar
\((\lambda u)' = \lambda u'\)

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] Ces propriétés s’étendent aux fonctions vectorielles en dimension
supérieure.
\item[] \sphinxstylestrong{Fonctions vectorielles.}
\item[] Soit \(f: \mathcal{O}\rightarrow \mathbb{R}^p\) une fonction, avec
\(\mathcal{O}\) une partie ouverte de
\(\mathbb{R}^n, n, p\ge 1\).
\item[] On dit que \(f\) est \sphinxstylestrong{différentiable} (au sens de Fréchet) en
\(\mathbf{a}\in \mathcal{O}\), s’il existe une application
linéaire continue \(L: \mathbb{R}^n\rightarrow \mathbb{R}^p\)
telle que pour tout \(h\in \mathbb{R}^n\), on a
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:27}
\begin{split}\begin{aligned}
    \lim_{h\rightarrow 0} \frac{f(\mathbf{a}+h)-f(\mathbf{a})-L(h)}{\|h\|}=0.\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Si \(f\) est différentiable en tout point de \(\mathcal{O}\),
on dit que \(f\) est différentiable sur \(\mathcal{O}\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] La différentielle de \(f\) est notée \(Df\).
\item[] \sphinxstylestrong{Dérivées partielles.}
\item[] Soient
\(\mathbf{a}=\begin{bmatrix} a_1\\ a_2\\ \vdots \\ a_n \end{bmatrix}\in \mathcal{O}\subseteq{\mathbb{R}^n}\)
et \(f: \mathcal{O}\rightarrow \mathbb{R}^p\) une fonction.
\item[] On dit que \(f\) admet une dérivée partielle par rapport à la
\(j-ème\) variable \(x_j\) si la limite:
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:28}
\begin{split}\begin{aligned}
    \lim_{h\rightarrow 0}\frac{f(a_1, a_2, \dots, a_j+h, \dots, a_n)-f(\mathbf{a})}{h} \text{ est finie.}\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
La dérivée partielle par rapport à la variable \(x_j\) de
\(f\) en \(\mathbf{a}\) est notée
\(\frac{\partial f}{\partial x_j}(\mathbf{a})\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Note.} Si \(f\) est différentiable, alors \(f\) admet des
dérivées partielles par rapport à toutes les variables.
\item[] \sphinxstylestrong{Gradient et Matrice Jacobienne.} Soit
\(f: \mathcal{O}\subseteq\mathbb{R}^n \rightarrow \mathbb{R}^p\)
une fonction différentiable.
\item[] On suppose que les fonctions composantes de \(f\) sont
\(f_1, f_2, \dots, f_p\).
\item[] Alors la matrice des dérivées partielles
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:29}
\begin{split}\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} &\dots & \frac{\partial f_1}{\partial x_n}\\[0.2cm]
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} &\dots & \frac{\partial f_2}{\partial x_n}\\
\vdots&\vdots& & \vdots\\\vspace{0.2cm}
\frac{\partial f_p}{\partial x_1} & \frac{\partial f_p}{\partial x_2} &\dots & \frac{\partial f_p}{\partial x_n}
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
est appelée la \sphinxstylestrong{matrice jacobienne} de \(f\), notée
\(\mathbf{J}_f\) ou \(\mathbf{J}(f)\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] Dans le cas \(p=1\), le vecteur
\(\begin{bmatrix} \frac{\partial f}{\partial x_1} \\[0.2cm] \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}\)
est appelé \(\operatorname{\mathbf{gradient}}\) de \(f\) et
noté \(\mathbf{\nabla} f\) ou
\(\operatorname{\mathbf{grad}}(f)\).
\item[] \sphinxstylestrong{Exemples du calcul de dérivées et de gradients sur
:math:`mathbb\{R\}\textasciicircum{}n`.}
\end{DUlineblock}
\begin{itemize}
\item {} 
\begin{DUlineblock}{0em}
\item[] \(f(\mathbf{x})=\big{<}\mathbf{x},\mathbf{x}\big{>}=\mathbf{x}^T\mathbf{x}\).
Le gradient de \(f\) est
\(\nabla f(\mathbf{x})=2\mathbf{x}\)
\end{DUlineblock}

\item {} 
\begin{DUlineblock}{0em}
\item[] \(f(\mathbf{x})=\mathbf{A}\mathbf{x}+\mathbf{b}\), avec
\(\mathbf{A}\) une matrice et \(\mathbf{b}\) un vecteur. On
a \(Df(\mathbf{x})=\mathbf{A}.\)
\end{DUlineblock}

\end{itemize}


\paragraph{Dérivées de fonctions composées.}
\label{\detokenize{chapter2:derivees-de-fonctions-composees}}
\begin{DUlineblock}{0em}
\item[] Il existe souvent des fonctions dont le gradient ne peut facilement
être calculé en utilisant les formules précédentes. Pour trouver le
gradient d’une telle fonction, on va réécrire la fonction comme étant
une composition de fonctions dont le gradient est facile à calculer en
utilisant les techniques que nous allons introduire. Dans cette partie
nous allons présenter trois formules de dérivation de fonctions
composées.
\item[] \sphinxstylestrong{Composition de fonctions à une seule variable.}
\item[] Soit \(f,g,h: \mathbb{R}\rightarrow\mathbb{R}\), trois fonctions
réelles telles que \(f(x) = g(h(x))\).
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:30}
\begin{split}\frac{\mathrm{d} f}{\mathrm{d} x} = \frac{\mathrm{d} g}{\mathrm{d} h}\frac{\mathrm{d} h}{\mathrm{d} x}\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstylestrong{Formule de dérivée totale.}
\end{quote}

\begin{DUlineblock}{0em}
\item[] Soit \(f:\mathbb{R}^{n+1}\rightarrow\mathbb{R}\) telle que
\(f = f(x,u_1(x),\dots,u_n(x))\) avec
\(u_i:\mathbb{R}\rightarrow\mathbb{R}\) alors
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:31}
\begin{split}\frac{\mathrm{d} f\left(x, u_{1}, \ldots, u_{n}\right)}{\mathrm{d} x}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial u_{1}} \frac{\mathrm{d} u_{1}}{\mathrm{d} x}+\frac{\partial f}{\partial u_{2}} \frac{\mathrm{d} u_{2}}{\mathrm{d} x}+\ldots+\frac{\partial f}{\partial u_{n}} \frac{\mathrm{d} u_{n}}{\mathrm{d} x}=\frac{\partial f}{\partial x}+\sum_{i=1}^{n} \frac{\partial f}{\partial u_{i}} \frac{\mathrm{d} u_{i}}{\mathrm{d} x}.\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Formule générale de dérivées de fonctions composées.}
\item[] Soit
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:32}
\begin{split}\begin{array}{l}
f :\mathbb{R}^{k} \rightarrow\mathbb{R}^{m} \\
\mathbf{x} \mapsto f(\mathbf{x})
\end{array}
\begin{array}{l}
g :\mathbb{R}^{n} \rightarrow\mathbb{R}^{k} \\
\mathbf{x} \mapsto g(\mathbf{x})
\end{array}\end{split}
\end{equation}
\sphinxAtStartPar
où \(\mathbf{x}=(x_1,\dots,x_n)\) ,
\(f(\mathbf{x}) = (f_1(\mathbf{x}),\dots,f_m(\mathbf{x}))\) et
\(g(\mathbf{x}) = (g_1(\mathbf{x}),\dots,g_k(\mathbf{x}))\).
\end{quote}

\sphinxAtStartPar
Le gradient de \(f(g(\mathbf{x}))\) est défini comme suit:
\begin{equation}\label{equation:chapter2:chapter2:33}
\begin{split}\frac{\partial}{\partial \mathbf{x}} \mathbf{f}(\mathbf{g}(\mathbf{x}))=\left[\begin{array}{cccc}
\frac{\partial f_{1}}{\partial g_{1}} & \frac{\partial f_{1}}{\partial g_{2}} & \ldots & \frac{\partial f_{1}}{\partial g_{k}} \\
\frac{\partial f_{2}}{\partial g_{1}} & \frac{\partial f_{2}}{\partial g_{2}} & \cdots & \frac{\partial f_{2}}{\partial g_{k}} \\
\vdots &\vdots & \ddots & \vdots \\
\frac{\partial f_{m}}{\partial g_{1}} & \frac{\partial f_{m}}{\partial g_{2}} & \ldots & \frac{\partial f_{m}}{\partial g_{k}}
\end{array}\right]\left[\begin{array}{cccc}
\frac{\partial g_{1}}{\partial x_{1}} & \frac{\partial g_{1}}{\partial x_{2}} & \ldots & \frac{\partial g_{1}}{\partial x_{n}} \\
\frac{\partial g_{2}}{\partial x_{1}} & \frac{\partial g_{2}}{\partial x_{2}} & \cdots & \frac{\partial g_{2}}{\partial x_{n}} \\
\vdots &\vdots & \ddots & \vdots \\
\frac{\partial g_{k}}{\partial x_{1}} & \frac{\partial g_{k}}{\partial x_{2}} & \cdots & \frac{\partial g_{k}}{\partial x_{n}}
\end{array}\right]\end{split}
\end{equation}

\subsubsection{Probabilités}
\label{\detokenize{chapter2:probabilites}}
\begin{DUlineblock}{0em}
\item[] La théorie des probabilités constitue un outil fondamental dans
l’apprentissage automatique. Les probabilités vont nous servir à
modéliser une expérience aléatoire, c’est\sphinxhyphen{}à\sphinxhyphen{}dire un phénomène dont on
ne peut pas prédire l’issue avec certitude, et pour lequel on décide
que le dénouement sera le fait du hasard.
\item[] \sphinxstylestrong{Définition.}
\item[] Une probabilité est une application sur \(\mathcal{P}(\Omega),\)
l’ensemble des parties de \(\Omega\) telle que:
\end{DUlineblock}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(0 \leq \operatorname{\mathbb{P}}(A) \leq 1,\) pour tout
événement \(A \subseteq \Omega\);

\item {} 
\sphinxAtStartPar
\(\operatorname{\mathbb{P}}(A)=\sum_{\{\omega\} \in A} \operatorname{\mathbb{P}}(\omega),\)
pour tout événement \(A\);

\item {} 
\sphinxAtStartPar
\(\operatorname{\mathbb{P}}(\Omega)=\sum_{A_{i}} \operatorname{\mathbb{P}}(A_{i})=1,\)
avec les \(A_{i} \subseteq \Omega\) une partition de
\(\Omega\).

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Proposition.} Soient \(A\) et \(B\) deux événements,
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\operatorname{Si} A\) et \(B\) sont incompatibles,
\(\operatorname{\mathbb{P}}(A \cup B)=\operatorname{\mathbb{P}}(A)+\operatorname{\mathbb{P}}(B).\)

\item {} 
\sphinxAtStartPar
\(\operatorname{\mathbb{P}}\left(A^{c}\right)=1-\operatorname{\mathbb{P}}(A)\),
avec \(A^{c}\) le complémentaire de \(A\).

\item {} 
\sphinxAtStartPar
\(\operatorname{\mathbb{P}}(\emptyset)=0.\)

\item {} 
\sphinxAtStartPar
\(\operatorname{\mathbb{P}}(A \cup B)=\operatorname{\mathbb{P}}(A)+\operatorname{\mathbb{P}}(B)-\operatorname{\mathbb{P}}(A \cap B).\)

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] Preuve voir {[}@epardoux{]}
\item[] Ci\sphinxhyphen{}dessous une définition plus générale de probabilité, valable pour
des espaces des événements possibles non dénombrables.
\item[] \sphinxstylestrong{Définition.} Soit \(A\) une expérience alátoire et
\(\Omega\) l’espace des événements possibles associés. Une
probabilité sur \(\Omega\) est une application définie sur
l’ensemble des événements, qui vérifie:
\end{DUlineblock}

\sphinxAtStartPar
::: \{.center\}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Axiome 1:} \(0\leq \operatorname{\mathbb{P}}(A)\leq 1\), pour
tout événement \(A\);

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Axiome 2:} Pour toute suite d’événements
\((A_i)_{i\in \operatorname{\mathbf{N}}}\), deux à deux
incompatibles,
\begin{equation}\label{equation:chapter2:chapter2:34}
\begin{split}\operatorname{\mathbb{P}}\left(\bigcup_{i \in \operatorname{\mathbf{N}}} A_{i}\right)=\sum_{i \in \operatorname{\mathbf{N}}} \operatorname{\mathbb{P}}\left(A_{i}\right);\end{split}
\end{equation}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Axiome 3:} \(\operatorname{\mathbb{P}}(\Omega) = 1.\) ::

\end{itemize}

\sphinxAtStartPar
\(\mathrm{NB}:\) Les événements
\(\left(A_{i}\right)_{i \in \operatorname{\mathbf{N}}}\) sont deux à
deux incompatibles, si pour tous
\(i \neq j, A_{i} \cap A_{j}=\emptyset\).


\paragraph{Indépendance et conditionnement.}
\label{\detokenize{chapter2:independance-et-conditionnement}}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Motivation.}
\item[] Quelle est la probablité d’avoir un cancer du poumon?
\item[] Information supplémentaire: vous fumez une vingtaine de cigarettes par
jour. Cette information va changer la probabilité. L’outil qui permet
cette mise à jour est la probabilité conditionnelle.
\item[] \sphinxstylestrong{Définition.}
\item[] Étant donnés deux événements \(A\) et \(B\), avec
\(\operatorname{\mathbb{P}}(A) > 0\), on appelle probabilité de
\(B\) conditionnellement à \(A\), ou sachant \(A,\) la
probabilité notée \(\operatorname{\mathbb{P}}(B \mid A)\) définie
par:
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:35}
\begin{split}\operatorname{\mathbb{P}}(B \mid A)=\frac{\operatorname{\mathbb{P}}(A \cap B)}{\mathbb{P}(A)}.\end{split}
\end{equation}
\sphinxAtStartPar
L’équation {\hyperref[\detokenize{chapter2:prob_condit}]{\emph{{[}prob\_condit{]}}}} (\autopageref*{\detokenize{chapter2:prob_condit}}) peut aussi s’écrire
comme \(\mathbb{P}(A \cap B)=\mathbb{P}(B \mid A) \mathbb{P}(A)\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] De plus, la probabilité conditionnelle sachant \(A\), notée
\(\mathbb{P}(. \mid A)\) est une nouvelle probabilité et possède
toutes les propriétés d’une probabilité.
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Proposition.} Formule des probabilités totales généralisée
\item[] Soit \((A_i)_{i\in I}\) (\(I\) un ensemble fini d’indices) une
partition de \(\Omega\) telle que
\(0 < \mathbb{P}(A_i)\leq 1 \quad\forall\  i\in I.\) Pour tout
événement B, on a
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:36}
\begin{split}\mathbb{P}(B)=\sum_{i \in I} \mathbb{P}\left(B|A_{i}\right)\mathbb{P}\left(A_{i}\right).\end{split}
\end{equation}
\sphinxAtStartPar
La formule des probabilités totales permet de servir les étapes de
l’expérience aléatoire dans l’ordre chronologique. \sphinxstylestrong{Proposition.}
Formule de Bayes généralisée
\end{quote}

\begin{DUlineblock}{0em}
\item[] Soit \((A_i)_{i\in I}\) une partition de \(\Omega\) tel que
\(0\leq \mathbb{P}(A_{i})\leq 1,\forall\  i\in I\). Soit un
événement \(B\), tel que \(\mathbb{P}(B)>0\). Alors pour tout
\(i\in I\),
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:37}
\begin{split}\mathbb{P}(A_{i}|B)=\frac{ \mathbb{P}\left(B|A_{i}\right)\mathbb{P}\left(A_{i}\right)}{\sum_{i \in I} \mathbb{P}\left(B|A_{i}\right)\mathbb{P}\left(A_{i}\right)}.\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] Deux événements \(A\) et \(B\) sont dits \sphinxstylestrong{indépendants} si
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:38}
\begin{split}\mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B).\end{split}
\end{equation}
\sphinxAtStartPar
S’ils sont de probabilité non nulle, alors
\begin{equation}\label{equation:chapter2:chapter2:39}
\begin{split}\mathbb{P}(B|A) = \mathbb{P}(B) \Leftrightarrow \mathbb{P}(A|B) = \mathbb{P}(A) \Leftrightarrow \mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B).\end{split}
\end{equation}\end{quote}


\paragraph{Variables aléatoires.}
\label{\detokenize{chapter2:variables-aleatoires}}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] Une variable aléatoire (v.a) \(X\) est une fonction définie sur
l’espace fondamental \(\Omega\), qui associe une valeur numérique
à chaque résultat de l’expérience aléatoire étudiée. Ainsi, à chaque
événement élémentaire \(\omega\), on associe un nombre
\(X(\omega)\).
\end{DUlineblock}

\sphinxAtStartPar
Une variable qui ne prend qu’un nombre dénombrable de valeurs est dite
\sphinxstylestrong{discrète} (par exemple le résultat d’une lancée d’une pièce de
monnaie, …), sinon, elle est dite \sphinxstylestrong{continue} (par exemple le prix
d’un produit sur le marché au fil du temps, distance de freinage d’une
voiture roulant à 100 km/h).


\paragraph{Variable aléatoire discrète}
\label{\detokenize{chapter2:variable-aleatoire-discrete}}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] L’espérance mathématique ou moyenne d’une v.a discrète \(X\) est
le réel
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:40}
\begin{split}\mathbb{E}[X] = \sum_{k=0}^{\infty} k \mathbb{P}[X = k].\end{split}
\end{equation}\end{quote}

\sphinxAtStartPar
Pour toute fonction \(g\),
\begin{equation}\label{equation:chapter2:chapter2:41}
\begin{split}\mathbb{E}[g(X)] = \sum_{k=0}^{\infty} g(k) \mathbb{P}[X = k].\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] La variance d’une v.a discrète \(X\) est le réel positif
\end{DUlineblock}
\begin{equation}\label{equation:chapter2:chapter2:42}
\begin{split}Var[X] = \mathbb{E}\left[(X-\mathbb{E}[X])^2\right] = \sum_{k=0}^{\infty} \left(k-\mathbb{E}[X]\right)^2 \mathbb{P}[X = k] = \mathbb{E}[X^2] -
\mathbb{E}[X]^2\end{split}
\end{equation}
\sphinxAtStartPar
et l’écart\sphinxhyphen{}type de \(X\) est la racine carrée de sa variance. \$\$

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Exemple:} Loi de Bernoulli
\item[] La loi de Bernoulli est fondamentale pour la modélisation des
problèmes de classification binaire en apprentissage automatique. On
étudie que les expériences aléatoires qui n’ont que deux issues
possibles (succès ou échec). Une expérience aléatoire de ce type est
appelée une épreuve de Bernoulli. Elle se conclut par un succès si
l’évènement auquel on s’intéresse est réalisé ou un échec sinon. On
associe à cette épreuve une variable aléatoire \(Y\) qui prend la
valeur \(1\) si l’évènement est réalisé et la valeur \(0\)
sinon. Cette v.a. ne prend donc que deux valeurs (\(0\) et
\(1\)) et sa loi est donnée par :
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:43}
\begin{split}\mathbb{P}[Y=1]=p, \quad \mathbb{P}[Y=0]=q=1-p.\qquad \operatorname{Avec } p \in [0, 1].\end{split}
\end{equation}\end{quote}

\sphinxAtStartPar
On dit alors que \(Y\) suit une loi de Bernoulli de paramètre
\(p,\) notée \(\mathcal{B}(p)\). La v.a. \(Y\) a pour
espérance \(p\) et pour variance \(p(1-p) .\) En effet,
\begin{equation}\label{equation:chapter2:chapter2:44}
\begin{split}\mathbb{E}[Y]=0 \times(1-p)+1 \times p=p\end{split}
\end{equation}
\sphinxAtStartPar
et
\begin{equation}\label{equation:chapter2:chapter2:45}
\begin{split}\operatorname{Var}(Y)=\mathbb{E}\left[Y^{2}\right]-\mathbb{E}[Y]^{2}=\mathbb{E}[Y]-\mathbb{E}[Y]^{2}=p(1-p).\end{split}
\end{equation}
\sphinxAtStartPar
\sphinxstylestrong{Schéma de Bernoulli} :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Chaque épreuve a deux issues : succès \([S]\) ou échec
\([E]\).

\item {} 
\sphinxAtStartPar
Pour chaque épreuve, la probabilité d’un succès est la même, notons
\(\mathbb{P}(S) = p\) et \(\mathbb{P}(E) = q = 1 - p.\)

\item {} 
\sphinxAtStartPar
Les \(n\) épreuves sont \sphinxstylestrong{indépendantes} : la probabilité d’un
succès ne varie pas, elle ne dépend pas des informations sur les
résultats des autres épreuves.

\end{itemize}


\paragraph{Variable aléatoire continue}
\label{\detokenize{chapter2:variable-aleatoire-continue}}
\sphinxAtStartPar
Contrairement aux v.a. discrètes, les v.a. continues sont utilisées pour
mesurer des grandeurs “continues” (comme distance, masse, pression…).
Une variable aléatoire continue est souvent définie par sa densité de
probabilité ou simplement densité. Une densité \(f\) décrit la loi
d’une v.a \(X\) en ce sens:
\begin{equation}\label{equation:chapter2:chapter2:46}
\begin{split}\forall\  a,b \in \mathbb{R}, \quad \mathbb{P}[a\leq X \leq b] = \int_{a}^{b} f(x)dx\end{split}
\end{equation}
\sphinxAtStartPar
et
\begin{equation}\label{equation:chapter2:chapter2:47}
\begin{split}\forall\  x\in \mathbb{R}, \quad F(x) = \mathbb{P}[X\leq x] =  \int_{-\infty}^{x} f(t)dt\end{split}
\end{equation}
\sphinxAtStartPar
. On en déduit qu’une densité doit vérifier
\begin{equation}\label{equation:chapter2:chapter2:48}
\begin{split}\forall\  x\in \mathbb{R}, \quad f(x)\geq 0 \text{ et } \int_{\mathbb{R}}^{} f(x)dx = 1\end{split}
\end{equation}
\sphinxAtStartPar
.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] On appelle densité de probabilité toute fonction réelle positive,
d’intégrale \(1\).
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] L’espérance mathématiques de la v.a \(X\) est définie par
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:49}
\begin{split}\mathbb{E}[X]=\int_{\mathbb{R}}^{} xf(x)dx.\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Exemple.} \sphinxstylestrong{La loi normale}
\item[] C’est la loi de probabilité la plus importante. Son rôle est central
dans de nombreux modèles probabilistes et en statistique. Elle possède
des propriétés intéressantes qui la rendent agréable à utiliser. La
densité d’une variable aléatoire suivant la loi normale de moyenne
\(\mu\) et d’écart\sphinxhyphen{}type \(\sigma\)
(\(\mathcal{N}(\mu,\sigma^2)\)) est définie par
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:50}
\begin{split}f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right), \quad \forall\  x \in \mathbb{R}.\end{split}
\end{equation}
\sphinxAtStartPar
Quand \(\mu=0 \quad \text{et} \quad \sigma = 1\), on parle de loi
normale centrée et réduite.
\end{quote}


\paragraph{Loi des grands nombres}
\label{\detokenize{chapter2:loi-des-grands-nombres}}
\sphinxAtStartPar
Considérons une suite \(\left(X_{n}\right)_{n \geq 1}\) de v.a.
indépendantes et de même loi. Supposons que ces v.a. ont une espérance,
\(m\) et une variance, \(\sigma^{2}\).

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Théorème.}
\item[] 
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:51}
\begin{split}\mathbb{E}\left[\sum_{i=1}^{n} X_i\right] = nm\end{split}
\end{equation}\begin{equation}\label{equation:chapter2:chapter2:52}
\begin{split}\operatorname{Var}\left[\sum_{i=1}^{n} X_i\right] = n\sigma^2\end{split}
\end{equation}\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] La moyenne empirique des v.a. \(X_1,\dots,X_n\) est la v.a.
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:53}
\begin{split}\bar{X_n} = \frac{X_1+\dots + X_n}{n}.\end{split}
\end{equation}
\sphinxAtStartPar
On sait d’ores et déjà que la moyenne empirique a pour espérance
\(m\) et pour variance \(\frac{\sigma^2}{n}\). Ainsi, plus
\(n\) est grand, moins cette v.a. varie. A la limite, quand
\(n\) tend vers l’infini, elle se concentre sur son espérance,
\(m\). C’est la loi des grands nombres.
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Théorème.} Convergence en Probabilité
\item[] Quand \(n\) est grand, \(\bar{X_n}\) est proche de \(m\)
avec une forte probabilité. Autrement dit,
\(\forall\ \varepsilon \ge 0, \quad \lim\limits\_{n\to\infty} \mathbb{P}(|\bar{X_n}-m|> \varepsilon) = 0.\)
\end{DUlineblock}

\sphinxAtStartPar
\sphinxstylestrong{Théorème central limite} Le Théorème central limite est très
important en apprentissage automatique. Il est souvent utilisé pour la
transformation des données surtout au traitement de données aberrantes.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Théorème.}
\item[] Pour tous réels \(a<b\), quand \(n\) tend vers
\(+\infty\),
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:54}
\begin{split}\mathbb{P}\left(a \leq \frac{\bar{X}_{n}-m}{\sigma / \sqrt{n}} \leq b\right) \longrightarrow \int_{a}^{b} \frac{1}{\sqrt{2 \pi}} e^{-x^{2} / 2} \mathrm{d} x.\end{split}
\end{equation}
\sphinxAtStartPar
On dit que
\(\displaystyle \frac{\bar{X}_{n}-m}{\sigma / \sqrt{n}}\) converge
en loi vers la loi normale \(\mathcal{N}(0,1)\).
\end{quote}


\paragraph{Intervalles de confiance}
\label{\detokenize{chapter2:intervalles-de-confiance}}
\sphinxAtStartPar
Soit \(X\) un caractère (ou variable) étudié sur une population, de
moyenne \(m\) et de variance \(\sigma^2\). On cherche ici à
donner une estimation de la moyenne \(m\) de ce caractère, calculée
à partir de valeurs observées sur un échantillon
\((X_1, ..., X_n)\). La fonction de l’échantillon qui estimera un
paramètre est appelée estimateur, son écart\sphinxhyphen{}type est appelé erreur
standard et est noté SE. L’estimateur de la moyenne \(m\) est la
moyenne empirique:
\begin{equation}\label{equation:chapter2:chapter2:55}
\begin{split}\frac{1}{n}\sum_{i=1}^{n} X_i\end{split}
\end{equation}
\sphinxAtStartPar
D’après les propriétés de la loi normale, avec un erreur
\(\alpha = 5\%\) quand \(n\) est grand on sait que
\begin{equation}\label{equation:chapter2:chapter2:56}
\begin{split}\mathbb{P}\left[m-2\sigma/ \sqrt{n}\leq \bar{X_n}\leq m+2\sigma/ \sqrt{n}\right] = 1- \alpha = 0.954\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] ou, de manière équivalente,
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:57}
\begin{split}\mathbb{P}\left[\bar{X_n}-2\sigma/ \sqrt{n}\leq m\leq \bar{X_n}+2\sigma/ \sqrt{n}\right] = 1- \alpha = 0.954\end{split}
\end{equation}
\sphinxAtStartPar
Ce qui peut se traduire ainsi: quand on estime \(m\) par
\(\bar{X_n}\), l’erreur faite est inférieure à
\(2\sigma/ \sqrt{n}\), pour \(95,4\%\) des échantillons. Ou
avec une probabilité de \(95,4\%\), la moyenne inconnue \(m\)
est dans l’intervalle
\(\left[\bar{X_n}-2\sigma/ \sqrt{n},\ \bar{X_n}+2\sigma/ \sqrt{n}\right]\).
\end{quote}

\begin{DUlineblock}{0em}
\item[] Voir {[}@estatML{]} pour plus d’explication.
\item[] \sphinxstylestrong{Définition.}
\item[] On peut associer à chaque incertitude \(\alpha\), un intervalle
appelé intervalle de confiance de niveau de confiance
\(1 - \alpha\), qui contient la vraie moyenne \(m\) avec une
probabilité égale à \(1 - \alpha\).
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] Soit \(Z\) une v.a.. Le fractile supérieur d’ordre \(\alpha\)
de la loi de \(Z\) est le réel \(z\) qui vérifie
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:58}
\begin{split}\mathbb{P}\left[Z\geq z\right] = \alpha\end{split}
\end{equation}
\sphinxAtStartPar
Le fractile inférieur d’ordre \(\alpha\) de la loi \(Z\) est
le réel \(z\) qui vérifie
\begin{equation}\label{equation:chapter2:chapter2:59}
\begin{split}\mathbb{P}\left[Z\leq z\right] = \alpha.\end{split}
\end{equation}
\sphinxAtStartPar
Quand l’écart\sphinxhyphen{}type théorique de la loi du caractère \(X\) étudié
n’est pas connu, on l’éstime par l’écart\sphinxhyphen{}type empirique
\(s_{n-1}\). Comme on dispose d’un grand échantillon, l’erreur
commise est petite. L’intervalle de confiance, de niveau de confiance
\(1-\alpha\) devient :
\begin{equation}\label{equation:chapter2:chapter2:60}
\begin{split}\left[\bar{x}_{n}-z_{\alpha / 2} \frac{s_{n-1}}{\sqrt{n}}, \ \bar{x}_{n}+z_{\alpha / 2} \frac{s_{n-1}}{\sqrt{n}}\right]\end{split}
\end{equation}
\sphinxAtStartPar
où
\begin{equation}\label{equation:chapter2:chapter2:61}
\begin{split}s_{n-1}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.\end{split}
\end{equation}\end{quote}


\subsubsection{Estimations paramétriques}
\label{\detokenize{chapter2:estimations-parametriques}}
\begin{DUlineblock}{0em}
\item[] Soit \((\Omega,\mathcal{A},\mathbf{P})\) un espace probabilisé et
\(\mathbf{x}\) une \(v.a.\) de \((\Omega,\mathcal{A})\)
dans \((E,\mathcal{E})\). La donnée d’un modèle statistique c’est
la donnée d’une famille de probabilités sur \((E,\mathcal{E})\),
\(\{\mathbb{P}_{\theta},\theta\in\Theta\}\). Le modèle étant
donné, on suppose alors que la loi de \(\mathbf{x}\) appartient au
modèle \(\{\mathbf{P}_{\theta},\theta\in\Theta\}\). Par exemple
dans le modèle de Bernoulli, \(\mathbf{x} = (x_1,\dots,x_n)\) où
les \(x_i\) sont \(i.i.d.\) (indépendantes et identiquement
distribuées) de loi de Bernoulli de paramètre
\(\theta\in\left]0,1\right]\). \(E = \{0,1\}^n\),
\(\mathcal{E} = \mathcal{P}(E)\),
\(\Theta = \left]0,1\right]\) et
\(P_{\theta}=\left((1-\theta) \delta_{0}+\theta \delta_{1}\right)^{\otimes n}.\)
\item[] \sphinxstylestrong{Définition.}
\item[] On dit que le modèle
\(\left\{\mathbb{P}_{\theta},\theta\in\Theta\right\}\) est
identifiable si l’application
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:62}
\begin{split}\begin{array}{l}
        \Theta \rightarrow\left\{P_{\theta}, \theta \in \Theta\right\} \\
        \theta \mapsto P_{\theta}
    \end{array}\end{split}
\end{equation}
\sphinxAtStartPar
est injective.
\end{quote}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Définition.}
\item[] Soit \(g:\  \Theta\rightarrow\mathbb{R}^k\). On appelle estimateur
de \(g(\theta)\) au vu de l’observation \(x\), toute
application \(T : \Omega\rightarrow \mathbb{R}^k\) de la forme
\(T = h(x)\) où \(h : E\mapsto\mathbb{R}^k\) mesurable. Un
estimateur ne doit pas dépendre de la quantité \(g(\theta)\) que
l’on cherche à estimer. On introduit les propriètés suivantes d’un
estimateur.
\item[] \sphinxstylestrong{Définition.}
\item[] \(T\) est un estimateur sans biais de \(g(\theta)\) si pour
tout
\(\theta \in\Theta,\quad \mathbb{E}_{\theta}[T] = g(\theta)\).
\end{DUlineblock}

\sphinxAtStartPar
Dans le cas contraire, on dit que l’estimateur \(T\) est biaisé et
on appelle biais la quantité \(\mathbb{E}_{\theta}[T] - g(\theta)\).

\sphinxAtStartPar
Généralement \(\mathbf{x}\) est un vecteur
\(\left(x_{1}, \ldots, x_{n}\right)\) d’observations \((n\)
étant le nombre d’entre elles). Un exemple important est le cas où
\(x_{1},\ldots, x_{n}\) forme un \(n\)\sphinxhyphen{}échantillon c’est à dire
lorsque que \(x_{1}, \ldots, x_{n}\) sont i.i.d. On peut alors
regarder des propriétés asymptotiques de l’estimateur, c’est\sphinxhyphen{}à\sphinxhyphen{}dire en
faisant tendre le nombre d’observations \(n\) vers \(+\infty\).
Dans ce cas, il est naturel de noter \(T = T_n\) comme dépendant de
\(n\). On a alors la définition suivante :

\sphinxAtStartPar
\sphinxstylestrong{Définition.}

\sphinxAtStartPar
\(T_n\) est un estimateur consistant de \(g(\theta)\) si pour
tout \(\theta \in \Theta\), \(T_n\) converge en probabilité vers
\(g(\theta)\) sous \(P_{\theta}\) lorsque \(n\to\infty\).

\begin{DUlineblock}{0em}
\item[] On définit le risque quadratique de l’estimateur dans le cas où
\(g(\theta)\in\mathbb{R}\).
\item[] \sphinxstylestrong{Définition.}
\item[] Soit \(T_n\) un estimateur de \(g(\theta)\). Le risque
quadratique de \(T_n\) est défini par
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:63}
\begin{split}R(T_n,g(\theta)) = \mathbb{E}_{\theta}[(T_n - g(\theta))^2].\end{split}
\end{equation}\end{quote}


\paragraph{Estimation par la méthode des moments}
\label{\detokenize{chapter2:estimation-par-la-methode-des-moments}}
\sphinxAtStartPar
Considérons un échantillon \(\mathbf{x} = (x_1,\dots,x_n)\). Soit
\(f = (f_1,\dots,f_k)\) une application de \(\mathcal{X}\) dans
\(\mathbb{R}^k\) tel que le modèle
\(\{\mathbb{P}_{\theta},\theta\in\Theta\}\) est identifiable si
l’application \(\Phi\)
\begin{equation}\label{equation:chapter2:chapter2:64}
\begin{split}\begin{array}{l}
\Phi: ~\Theta \rightarrow \mathbb{R}^k \\
\quad ~~~\theta \mapsto \Phi(\theta) = \mathbb{E}_{\theta}[f(x)]
\end{array}\end{split}
\end{equation}
\sphinxAtStartPar
est injective. On définit l’estimateur \(\hat{\theta}_n\) comme la
solution dans \(\Theta\) (quand elle existe) de l’équation
\begin{equation}\label{equation:chapter2:chapter2:65}
\begin{split}\mathbb{E}_{\theta}[f(\mathbf{x})]\approx \frac{1}{n}\sum_{i=1}^{n} f(x_i).\end{split}
\end{equation}
\sphinxAtStartPar
Souvent, lorsque \(\mathcal{X}\subset \mathbb{R},\) on prend
\(f_i(x) = x^i\) et \(\Phi\) correspond donc au ième moment de
la variable de \(X_i\) sous \(\mathbb{P}_{\theta}\). Ce choix
justifie le nom donné à la méthode. Voici quelques exemples
d’estimateurs bâtis sur cette méthode.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Exemple.} Loi uniforme
\item[] Ici \(k=1\), \(Q_{\theta}\) est la loi uniforme sur
\([0,\theta]\) avec \(\theta > 0\). On a pour tout
\(\theta\),
\(\displaystyle \mathbb{E}_{\theta}[X_1] = \frac{\theta}{2}\), on
peut donc prendre par exemple
\(\displaystyle \Phi(\theta) = \frac{\theta}{2}\) et
\(f(x) = x\). L’estimateur obtenu par la méthode des moments est
alors \(\hat{\theta}_n =2\bar{X_n}\). Cet estimateur est sans
biais et constant.
\item[] \sphinxstylestrong{Exemple.} Loi normale
\item[] Ici \(k=2\), on prend \(Q_{\theta} = \mathcal{N}(m,\sigma^2)\)
avec
\(\theta = (m,\sigma^2)\in\mathbb{R}\times\mathbb{R}_{+}^{*}\).
Pour tout \(\theta\), \(\mathbb{E}_{\theta}[X_1] = m\) et
\(\mathbb{E}_{\theta}[X_1^2] = m^2 + \sigma^2\) , on peut donc
prendre par exemple, \(f_1(x) = x\) et \(f_2(x) = x^2\) ce qui
donne \(\Phi(m,\ \sigma^2) = (m,m^2+\sigma^2)\). L’estimateur
obtenu par la méthode des moments vérifie
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:66}
\begin{split}\hat{m}_{n}=\bar{X}_{n} \text { et } \hat{m}_{n}^{2}+\hat{\sigma}_{n}^2=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2},\end{split}
\end{equation}
\sphinxAtStartPar
c’est\sphinxhyphen{}à\sphinxhyphen{}dire
\begin{equation}\label{equation:chapter2:chapter2:67}
\begin{split}\hat{\theta}_{n}=\left(\bar{X}_{n},\ \frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}_{n}\right)^{2}\right).\end{split}
\end{equation}
\sphinxAtStartPar
L’estimateur est consistant mais l’estimateur de la variance est
biaisé.
\end{quote}


\paragraph{Estimation par maximum de vraisemblance}
\label{\detokenize{chapter2:estimation-par-maximum-de-vraisemblance}}
\begin{DUlineblock}{0em}
\item[] Soit \(\{E,\ \mathcal{E},\ \{P_{\theta},\ \theta\in\Theta\}\}\) un
modèle statistique, où \(\Theta\subset\mathbb{R}^k\). On suppose
qu’il existe une mesure \(\sigma\)\sphinxhyphen{}finie \(\mu\) qui domine le
modèle, c’est à dire que \(\forall\  \theta\in\Theta\),
\(P_{\theta}\) admet une densité par rapport à \(\mu\).
\item[] \sphinxstylestrong{Définition.}
\item[] Soit \(\mathbf{x}\) une observation. On appelle vraisemblance de
\(\mathbf{x}\) l’application
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:68}
\begin{split}\begin{array}{l}
    \Theta \rightarrow\mathbb{R}_+ \\
    \theta \mapsto \mathbb{P}(\theta,\ \mathbf{x})
    \end{array}\end{split}
\end{equation}
\sphinxAtStartPar
On appelle estimateur du maximum de vraisemblance de \(\theta\),
tout élément \(\hat{\theta}\) de \(\Theta\) maximisant la
vraisemblance, c’est à dire vérifiant
\begin{equation}\label{equation:chapter2:chapter2:69}
\begin{split}\hat{\theta} = \arg \underset{\theta\in\Theta}{\max}\  \mathbf{P}(\theta,\ \mathbf{x})\end{split}
\end{equation}\end{quote}

\sphinxAtStartPar
Considérons le cas typique où
\(\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right),\) les \(x_{i}\)
formant un \(n\)\sphinxhyphen{}échantillon de loi \(Q_{\theta_{0}}\) où
\(Q_{\theta_{0}}\) est une loi sur \(\mathcal{X}\) de paramètre
inconnu \(\theta_{0} \in \Theta \subset\) \(\mathbb{R}^{k} .\)
On suppose en outre que pour tout \(\theta \in \Theta, Q_{\theta}\)
est absolument continue par rapport à une mesure \(\nu\) sur
\(\mathcal{X}\). Dans ce cas, en notant
\begin{equation}\label{equation:chapter2:chapter2:70}
\begin{split}q(\theta, x)=\frac{d Q_{\theta}}{d \nu}(x)\end{split}
\end{equation}
\sphinxAtStartPar
et en prenant \(\mu=\nu^{\otimes n}\) on a la vraisemblance qui
s’écrit sous la forme
\begin{equation}\label{equation:chapter2:chapter2:71}
\begin{split}\mathbb{P}(\theta, \mathbf{x})=\prod_{i=1}^{n} q\left(\theta, x_{i}\right)\end{split}
\end{equation}
\sphinxAtStartPar
et donc
\begin{equation}\label{equation:chapter2:chapter2:72}
\begin{split}\hat{\theta}_{n}=\arg \max _{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^{n} \log \left[q\left(\theta, x_{i}\right)\right]\end{split}
\end{equation}
\sphinxAtStartPar
avec la convention \(\log (0)=-\infty .\)

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Exemple.} Modèle de Bernoulli
\item[] Soit \(Q_{\theta_{0}}=\mathcal{B}(\theta)\) avec
\(\theta \in[0,1]=\Theta\). Pour tout \(\theta \in] 0,1[\) et
\(x \in\{0,1\}\)
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter2:chapter2:73}
\begin{split}q(\theta, x)=\theta^{x}(1-\theta)^{1-x}=(1-\theta) \exp \left[x \log \left(\frac{\theta}{1-\theta}\right)\right]\end{split}
\end{equation}
\sphinxAtStartPar
et donc l’estimateur du maximum de vraisemblance doit maximiser dans
l’intervalle {[}0,1{]}.
\begin{equation}\label{equation:chapter2:chapter2:74}
\begin{split}\log \left(\theta^{S_{n}}(1-\theta)^{n-S_{n}}\right)=S_{n} \log \left(\frac{\theta}{1-\theta}\right)+n \log (1-\theta)\end{split}
\end{equation}
\sphinxAtStartPar
avec \(S_n = \sum_{i} x_i\) ce qui conduit à
\(\hat{\theta}_{n}=\bar{\mathbf{x}}\) en résolvant l’équation
\(\nabla \log(q(\theta, x)) = 0\).
\end{quote}


\bigskip\hrule\bigskip



\subsection{Commentaires et Discussions}
\label{\detokenize{chapter2:commentaires-et-discussions}}
\sphinxAtStartPar
Partagez vos questions, commentaires et expériences avec la communauté
IVIA\sphinxhyphen{}AF ! Utilisez la version en ligne
\sphinxhref{https://livre.ivia.africa/chapter2.html}{livre.ivia.africa}%
\begin{footnote}[8]\sphinxAtStartFootnote
\sphinxnolinkurl{https://livre.ivia.africa/chapter2.html}
%
\end{footnote}.

\sphinxAtStartPar
\sphinxstyleemphasis{Les commentaires sont modérés pour maintenir un environnement
d’apprentissage respectueux et constructif.}

\sphinxstepscope


\section{Apprentissage Supervisé}
\label{\detokenize{chapter3:apprentissage-supervise}}\label{\detokenize{chapter3:ch2}}\label{\detokenize{chapter3::doc}}
\sphinxAtStartPar
Dans ce chapitre, nous allons explorer l’apprentissage supervisé. Ce
type d’apprentissage, aussi connu sous le nom d’apprentissage avec
tutelle (maître), permet de déterminer la relation qui existe entre une
variable explicative \(\mathbf{X}\) et une variable à expliquer
(étiquette) \(\mathbf{y}\). En d’autres termes, l’apprentissage
supervisé est le processus permettant à un modèle d’apprendre, en lui
fournissant des données d’entrée ainsi que des données de sortie. Cette
paire d’entrée/sortie est généralement appelée «données étiquetées».
Dans un cadre illustratif, pensez à un enseignant qui, connaissant la
bonne réponse à une question, évaluera un élève en fonction de
l’exactitude de sa réponse à cette question. Pour plus de clarification,
comparons l’approche de l’apprentissage automatique à la programmation
traditionnelle.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Dans la programmation traditionnelle, comme illustré dans la figure
{\hyperref[\detokenize{chapter3:fig:tradi_prog}]{\emph{1.1}}} (\autopageref*{\detokenize{chapter3:fig:tradi_prog}}), nous avons une fonction \(f\) connue
qui reçoit la donnée en entrée \(\mathbf{x}\) et renvoie la
réponse correspondante \(\mathbf{y}\) en sortie. Par exemple,
nous pouvons penser à écrire une fonction \(f\) qui calcule le
carré d’un nombre; si nous donnons en entrée le nombre \(2\),
notre programme va nous renvoyer la valeur
\(\displaystyle f(2) = 2^2 = 4 = y\).

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{traditional_programming}.png}
\caption{L’approche traditionnelle}\label{\detokenize{chapter3:id4}}\end{figure}

\item {} 
\sphinxAtStartPar
L’approche de la programmation utilisée dans l’apprentissage
automatique est tout à fait différente de la précédente. Dans cette
dernière, nous ne connaissons pas la fonction \(f\) et nous
voulons donc l’approximer par une fonction \(\hat{f}\) en
utilisant les données à notre disposition. Cette approche est donc
divisée en deux phases. La première est la phase où nous entraînons
notre fonction \(\hat{f}\) (figure {\hyperref[\detokenize{chapter3:fig:ML_prog_train}]{\emph{1.2}}} (\autopageref*{\detokenize{chapter3:fig:ML_prog_train}})).
Si nous revenons à notre exemple précédent, cette étape pourra
consister à présenter à la fonction \(\hat{f}\), plusieurs
couples de nombres et leurs carrés
\(\{(2, 4), (3, 9), (4, 16), \cdots\}\). L’objectif ici est de
trouver un moyen d’estimer la fonction “carrée” en observant
uniquement les données à notre disposition.

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ML_Programming_flow_Training}.png}
\caption{L’approche apprentissage automatique}\label{\detokenize{chapter3:id5}}\end{figure}

\sphinxAtStartPar
La dernière étape consiste à fournir un nouveau nombre à notre
fonction \(f^\star\), obtenue après l’étape \(1\), afin
qu’elle prédise (\sphinxstyleemphasis{approximativement}) le carré de ce nombre.

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{ML_Programming_flow_Testing}.png}
\caption{L’approche apprentissage automatique}\label{\detokenize{chapter3:id6}}\end{figure}

\sphinxAtStartPar
Dans la suite du cours, nous reviendrons beaucoup plus en détails sur
les étapes ci\sphinxhyphen{}dessus présentées.

\end{itemize}

\sphinxAtStartPar
L’apprentissage supervisé est souvent utilisé pour deux types de
problèmes: les problèmes de régression et les problèmes de
classification.


\subsection{Problèmes de Régression}
\label{\detokenize{chapter3:problemes-de-regression}}
\sphinxAtStartPar
Dans l’apprentissage supervisé, on parle de problèmes de régression
lorsque la variable à expliquer \(\mathbf{y}\) est continue. Par
exemple lorsqu’on veut \sphinxstylestrong{prédire le prix} d’une bouteille de vin sur la
base de \sphinxstylestrong{certaines variables} (le pays de fabrication, qualité, le
taux d’alcool, etc.). Il s’agit bel et bien d’un problème de régression.


\subsubsection{La Régression Linéaire}
\label{\detokenize{chapter3:la-regression-lineaire}}
\sphinxAtStartPar
La régression linéaire est un problème de régression pour lequel le
modèle ou la fonction dépend linéairement de ses paramètres {[}@reg\_lin{]}.
Les différents types de régression linéaire que nous connaissons sont la
régression linéaire affine, la régression linéaire polynomiale et la
régression linéaire à fonctions de base radiales. Dans ce document, nous
allons nous focaliser sur deux types fondamentaux de régression
linéaire: la régression linéaire affine et la régression linéaire
polynomiale.


\paragraph{La régression linéaire affine}
\label{\detokenize{chapter3:la-regression-lineaire-affine}}
\sphinxAtStartPar
Une régression linéaire de paramètre \(\boldsymbol{\theta}\) est
dite affine si pour tout \(\mathbf{x} \in \mathbb{R}^d.\)
\begin{equation}\label{equation:chapter3:chapter3:0}
\begin{split}f_{\boldsymbol{\theta}}(\mathbf{x}) = \boldsymbol{\theta}_{0} + \boldsymbol{\theta}_{1}^{T} \mathbf{x} = \begin{bmatrix} \boldsymbol{\theta}_0 & \boldsymbol{\theta}_1^{T} \end{bmatrix} \begin{bmatrix}
1 \\
\mathbf{x}
\end{bmatrix}\end{split}
\end{equation}
\sphinxAtStartPar
avec \(\boldsymbol{\theta}_0 \in \mathbb{R}\) et
\(\boldsymbol{\theta}_1 \in \mathbb{R}^d.\) Le terme
\(\left[ 1, \mathbf{x}\right]\) est appelé attribut du modèle et il
sera noté par \(\phi(\mathbf{x}).\)

\begin{sphinxuseclass}{center}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{linearReg_im1}.png}
\caption{Représentation graphique d’un exemple de données d’entraînement}\label{\detokenize{chapter3:id7}}\end{figure}

\end{sphinxuseclass}
\begin{DUlineblock}{0em}
\item[] Les jeux de données représentés dans la figure {\hyperref[\detokenize{chapter3:exdonnee}]{\emph{1.4}}} (\autopageref*{\detokenize{chapter3:exdonnee}})
forment un ensemble d’entraînement où la régression linéaire affine
sera la plus appropriée.
\item[] Pour déterminer les meilleurs paramètres de la régression linéaire
affine deux différentes méthodes sont utilisées à savoir: la méthode
explicite et la méthode approximative.
\end{DUlineblock}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{La méthode explicite}

\sphinxAtStartPar
Dans le cas de la régression linéaire affine, la méthode explicite
peut\sphinxhyphen{}être utilisée par le biais de l’estimation du maximum de
vraisemblance qui interpelle la notion de probabilité conditionnelle.

\begin{DUlineblock}{0em}
\item[] Pour être plus concret, nous allons considérer l’expression
suivante:
\item[] \(y_i = f_{\boldsymbol{\theta}} (\mathbf{x}_i) + \varepsilon~~~\)
avec \(\varepsilon \sim N(0, \sigma^2)\).
\end{DUlineblock}

\sphinxAtStartPar
Dans cette expression, nous supposons que \(f\) est la fonction
que nous allons estimer à partir de son paramètre
\(\boldsymbol{\theta}\) et qui nous permettra de faire nos
prédictions pour chaque élément donné à partir du domaine
d’entraînement. Nous noterons par \(\hat f\) comme étant la
fonction estimée de \(f\).

\sphinxAtStartPar
Pour une suite de points
\((\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2),..., (\mathbf{x}_n, y_n)\)
représentant le domaine d’entraînement nous supposons que les
\(y_i\) suivent chacun une loi normale et qu’ils sont aussi
indépendants et identiquement distribués (i.i.d).

\sphinxAtStartPar
Alors, nous avons
\(\mathbf{x} = \left\lbrace \mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n\right\rbrace \in \mathbb{R}^{n \times d}\)
et
\(\mathbf{y} = \left\lbrace y_1, y_2, ..., y_n\right\rbrace \in \mathbb{R}^{n}\).

\sphinxAtStartPar
Déterminons le paramètre \(\boldsymbol{\theta} ^{*}\) qui
maximise la vraisemblance.
\begin{equation}\label{equation:chapter3:chapter3:1}
\begin{split}\begin{aligned}
  \mathbb{P}(y_1, y_2,.., y_n|  \mathbf{x}_1, \mathbf{x}_2, \cdots \mathbf{x}_n; \boldsymbol{\theta}) &= \mathbb{P}(\mathbf{y}|\mathbf{x}; \boldsymbol{\theta})
  \\
  &= \prod_{i}^{n}\mathbb{P}(y_i| \mathbf{x}_i, \boldsymbol{\theta})~~ \text{avec }  y_i \sim N(\boldsymbol{\theta}^T \mathbf{x}_i; \sigma^2)\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Dans ce cas nous avons:
\begin{equation}\label{equation:chapter3:chapter3:2}
\begin{split}\mathbb{P}(y_i| \mathbf{x}_i; \boldsymbol{\theta}) = \frac{1}{\sigma \sqrt{2\pi }}\exp \left(-\frac{(yi - \boldsymbol{\theta}^T \mathbf{x_i})^2}{2 \sigma^2}\right).\end{split}
\end{equation}
\begin{DUlineblock}{0em}
\item[] Nous savons que, la fonction logarithme est une fonction
strictement croissante, ce qui implique que le paramètre
\(\boldsymbol{\theta} ^*\) qui maximise la vraisemblance
maximise aussi le logarithme\sphinxhyphen{}vraisemblance. Ainsi, en appliquant le
logarithme de la vraisemblance, nous avons:
\item[] 
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter3:chapter3:3}
\begin{split}\log \mathbb{P}(\mathbf{y}|  \mathbf{x}; \boldsymbol{\theta}) = \sum_{i=1}^{n} \log \mathbb{P}(y_i| \mathbf{x}_i; \boldsymbol{\theta}).\end{split}
\end{equation}\end{quote}

\sphinxAtStartPar
Pour chaque
\(i \in \left\lbrace 1, 2, ..., n\right\rbrace ,~\log \mathbb{P} (y_i| \mathbf{x}_i; \boldsymbol{\theta}) =\log\left(\frac{1}{\boldsymbol{\sigma} \sqrt{2\pi}}\right)~ -\frac{\left(y_i - \boldsymbol{\theta}^T \mathbf{x}_i\right)^2}{2 \sigma^2}\)
\begin{equation}\label{equation:chapter3:chapter3:4}
\begin{split}\begin{aligned}
\implies \log \mathbb{P} (\mathbf{y}| \mathbf{x}; \boldsymbol{\theta}) &= -\frac{1}{2 \sigma^2} \sum_{i=1}^{n}(y_i - \boldsymbol{\theta}^T \mathbf{x}_i)^2 +c^{ste}
\\
\\
&= -\frac{1}{2 \sigma^2} (\mathbf{y} - \boldsymbol{\theta} \mathbf{x})^T (\mathbf{y} - \boldsymbol{\theta} \mathbf{x}) + c^{ste}\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Ainsi, la dérivée partielle du logarithme de la vraisemblance par
rapport à \(\boldsymbol{\theta}\) est donnée par:
\begin{equation}\label{equation:chapter3:chapter3:5}
\begin{split}\begin{aligned}
\dfrac{\partial \log \mathbb{P} (\mathbf{y}| \mathbf{x}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} & = \frac{\partial}{\partial \boldsymbol{\theta}} \left( -\frac{1}{2 \sigma^2} (\mathbf{y} - \mathbf{x} \boldsymbol{\theta})^T (\mathbf{y} - \mathbf{x} \boldsymbol{\theta} ) + c^{ste} \right)
\\
\\
%& =  -\frac{1}{\sigma^2} (y - \boldsymbol{\theta} X)^T (y - \boldsymbol{\theta} X)
& = \frac{1}{\sigma^2} \mathbf{x}^T(\mathbf{x}\boldsymbol{\theta} - \mathbf{y}).\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Alors, résoudre l’équation:
\begin{equation}\label{equation:chapter3:chapter3:6}
\begin{split}\displaystyle \frac{\partial \log \mathbb{P}(\mathbf{y}| \mathbf{x}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0\end{split}
\end{equation}
\sphinxAtStartPar
nous permettra de trouver la valeur de \(\boldsymbol{\theta}^*\).
\begin{equation}\label{equation:chapter3:chapter3:7}
\begin{split}\begin{aligned}
    \frac{\partial \log \mathbb{P}(\mathbf{y}| \mathbf{x}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &=0,\\
    \mathbf{x}^T(\mathbf{x}\boldsymbol{\theta} - \mathbf{y}) &= 0, \\
    \mathbf{x}^T\mathbf{x}\boldsymbol{\theta} &= \mathbf{x}^T\mathbf{y}.\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
En supposant que la matrice \(\mathbf{x}^T\mathbf{x}\) est
inversible nous avons :
\begin{equation}\label{equation:chapter3:chapter3:8}
\begin{split}\boldsymbol{\theta}^{*} = (\mathbf{x}^T\mathbf{x})^{-1}\mathbf{x}^T\mathbf{y}.\end{split}
\end{equation}
\sphinxAtStartPar
Alors, vu que nous avons déterminé le paramètre
\(\boldsymbol{\theta}^{*}\), la fonction
\(\hat{\boldsymbol{f}}\) associée au paramètre
\(\boldsymbol{\theta}^{*}\), souvent appelée “hypothèse” ou
“modèle” s’écrit comme
\begin{equation}\label{equation:chapter3:chapter3:9}
\begin{split}\hat{\boldsymbol{f}}(\mathbf{x}) = \boldsymbol{\theta}^{*}\mathbf{x}.\end{split}
\end{equation}
\sphinxAtStartPar
{[}droite{]}

\begin{sphinxuseclass}{center}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{linearReg_im3}.png}
\caption{Représentation graphique de la fonction \(\hat{f}\) définie
dans l’ensemble \(X\) à valeur dans \(\mathbb{R}\).}\label{\detokenize{chapter3:id8}}\end{figure}

\end{sphinxuseclass}
\sphinxAtStartPar
La méthode explicite nous permet d’obtenir la solution exacte de
l’équation {\hyperref[\detokenize{chapter3:equat}]{\emph{{[}equat{]}}}} (\autopageref*{\detokenize{chapter3:equat}}). Tout de même, trouver cette solution
exacte est souvent très compliquée dans le cas où l’étude se fait sur
un grand ensemble de jeux de données (la complexité pour trouver
l’inverse dans l’équation {\hyperref[\detokenize{chapter3:star}]{\emph{{[}star{]}}}} (\autopageref*{\detokenize{chapter3:star}}) est
\(\mathcal{O}(n^{3})\)). Pour cela, dans ce qui suit, nous allons
présenter des méthodes alternatives qui nous permettront de donner
une valeur approchée à la solution exacte.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Méthodes approximatives}

\begin{DUlineblock}{0em}
\item[] Dans cette partie, nous allons utiliser une méthode itérative pour
estimer la valeur des paramètres de l’équation suivante:
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}
\begin{quote}
\begin{equation}\label{equation:chapter3:chapter3:10}
\begin{split}\mathbf{y}=\boldsymbol{\theta} \mathbf{x}\end{split}
\end{equation}
\sphinxAtStartPar
,
\end{quote}

\begin{DUlineblock}{0em}
\item[] où \(\boldsymbol{\theta} \in \mathbb{R}^{d+1}\) est le vecteur
de paramètres à estimer;
\(\mathbf{X} = \left\lbrace \mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n\right\rbrace \in \mathbb{R}^{n \times (d+1)}\)
et
\(\mathbf{y} = \left\lbrace y_1, y_2, ..., y_n\right\rbrace \in \mathbb{R}^{n}\)
les données. \$\$
\end{DUlineblock}

\sphinxAtStartPar
\sphinxstylestrong{La fonction de perte}

\sphinxAtStartPar
La fonction de perte mesure la différence entre la valeur observée et
la valeur estimée. En apprentissage automatique, l’objectif est
d’optimiser la fonction de perte. Il existe différentes fonctions de
perte selon le critère (ou métrique permettant d’évaluer la
performance du modèle) adopté(e). Dans cette partie, nous allons
utiliser l’erreur quadratique moyenne (appelé Mean Square Error (MSE)
en anglais) pour définir notre fonction de perte.

\sphinxAtStartPar
L’erreur quadratique moyenne entre le \(\mathbf{y}\) observé et
le \(\mathbb{\hat{y}}\) prédit est donnée par:
\begin{equation}\label{equation:chapter3:chapter3:11}
\begin{split}\begin{aligned}
\operatorname{MSE}(\mathbf{y}, \hat{\boldsymbol{y}}) & = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2,\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
où \(n\) est la dimension des vecteurs \(\mathbf{y}\) et
\(\hat{\boldsymbol{y}}\).

\sphinxAtStartPar
Dans le cas de la régression linéaire, cette fonction peut être
réécrite comme étant une fonction \(E\) de
\(\boldsymbol{\theta}\).
\begin{equation}\label{equation:chapter3:chapter3:12}
\begin{split}E\left(\boldsymbol{\theta}\right)  = \frac{1}{n}\sum_{i=1}^{n}(y_i - \boldsymbol{\theta}^{T} \mathbf{x}_i)^2.\end{split}
\end{equation}
\sphinxAtStartPar
Par conséquent, le paramètre \(\boldsymbol{\theta}\) qui
correspond à la meilleure ligne d’ajustement sera tout simplement la
valeur qui minimise la fonction de perte \(E\). Pour cela, nous
allons introduire une méthode la plus souvent utilisée pour minimiser
une fonction (éventuellement convexe) dans l’apprentissage
automatique à savoir la descente de gradient.

\sphinxAtStartPar
{[}f\_convexe{]}

\begin{sphinxuseclass}{center}
\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{linearReg_im2}.png}
\caption{Représentation graphique d’une fonction convexe}\label{\detokenize{chapter3:id9}}\end{figure}

\end{sphinxuseclass}
\end{itemize}


\paragraph{Descente de gradient \{GD\}}
\label{\detokenize{chapter3:descente-de-gradient-gd}}
\sphinxAtStartPar
La descente de gradient est une procédure itérative d’optimisation dans
laquelle, à chaque étape, on améliore la solution en essayant de
minimiser la fonction de perte considérée {[}@desc\_grad{]}. Elle est
appliquée lorsque l’on cherche le minimum d’une fonction dont on connaît
l’expression analytique, qui est dérivable, mais dont le calcul direct
du minimum est difficile.

\sphinxAtStartPar
Pour entamer cette procédure, nous allons commencer par initialiser le
paramètre \(\boldsymbol{\theta}\). Ensuite, nous calculons la
dérivée partielle de la fonction \(E\) par rapport au paramètre
\(\boldsymbol{\theta}\) donnée par:
\begin{equation}\label{equation:chapter3:chapter3:13}
\begin{split}\frac{\partial E}{\partial \boldsymbol{\theta}} = -\frac{2}{n} \sum_{i=1}^{n} \mathbf{x}_i(y_i - \boldsymbol{\theta}^{T} \mathbf{x}_i).\end{split}
\end{equation}
\sphinxAtStartPar
Pour trouver les meilleurs paramètres, nous allons répéter le processus
ci\sphinxhyphen{}dessous jusqu’à ce que la fonction de perte soit très proche ou égale
à \(0\).
\begin{equation}\label{equation:chapter3:chapter3:14}
\begin{split}\boldsymbol{\theta} = \boldsymbol{\theta} - \gamma \cdot \frac{\partial E}{\partial \boldsymbol{\theta}},\end{split}
\end{equation}
\sphinxAtStartPar
La valeur de \(\boldsymbol{\theta}\) trouvée après convergence est
la valeur optimale que nous noterons par \(\boldsymbol{\theta}^*\).

\sphinxAtStartPar
Alors, concernant l’exemple de la figure {\hyperref[\detokenize{chapter3:exdonnee}]{\emph{1.4}}} (\autopageref*{\detokenize{chapter3:exdonnee}}), notre
hypothèse ou modèle sera représenté par une droite d’ajustement de la
même forme que celle en couleur verte sur la figure
{\hyperref[\detokenize{chapter3:droitelin}]{\emph{1.5}}} (\autopageref*{\detokenize{chapter3:droitelin}}). Cette droite est d’équation:

\begin{sphinxuseclass}{center}
\sphinxAtStartPar
\(\mathbf{y} = \boldsymbol{\theta}^*\mathbf{x}\).

\end{sphinxuseclass}
\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{Implementation}
\end{DUlineblock}

\begin{sphinxuseclass}{algorithm}
\begin{sphinxuseclass}{algorithmic}
\sphinxAtStartPar
class \sphinxstylestrong{LinearRégression}():    def \sphinxstylestrong{\_\_init\_\_} (self):
      pass    def \sphinxstylestrong{fonction\_perte}(self, y\_vrai, y\_prédit):
      définit une fonction de perte et retourne sa valeur    def
\sphinxstylestrong{algorithme}(self, \(\mathbf{x}\), \(\mathbf{y}\),
taux\_apprentissage, nombre\_itération):       initialiser les
paramètres \(\boldsymbol{\theta}_0\) et
\(\boldsymbol{\theta}_1\)

\sphinxAtStartPar
for i in range(nombre\_itération):           \sphinxstylestrong{prédiction}(x),
          calcule la perte au moyen de \sphinxstylestrong{fonction\_perte},
          mise à jour des paramètres \(\boldsymbol{\theta}_0\)
et \(\boldsymbol{\theta}_1\),           return
\(\boldsymbol{\theta}_0\), \(\boldsymbol{\theta}_1\),
   def \sphinxstylestrong{prédiction}(self, \(\mathbf{x}\)):       y\_prédit
\(= \boldsymbol{\theta}_0^T \mathbf{x} + \boldsymbol{\theta}_1\),
      return y\_prédit

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Un exemple d’implementation de régression linéaire est disponible
\sphinxhref{https://colab.research.google.com/drive/1Ad94wJI2hch6BxpRV9P-SZcc3UfI2zwY\#scrollTo=BPyCNZ9Z6tMg}{ici}%
\begin{footnote}[9]\sphinxAtStartFootnote
\sphinxnolinkurl{https://colab.research.google.com/drive/1Ad94wJI2hch6BxpRV9P-SZcc3UfI2zwY\#scrollTo=BPyCNZ9Z6tMg}
%
\end{footnote}


\paragraph{La régression linéaire polynomiale}
\label{\detokenize{chapter3:la-regression-lineaire-polynomiale}}
\sphinxAtStartPar
La régression linéaire de paramètre \(\boldsymbol{\theta}\) est dite
polynomiale si pour tout \(\mathbf{x} \in \mathbb{R}^d,\)
\begin{equation}\label{equation:chapter3:chapter3:15}
\begin{split}\begin{aligned}
\text{f}_{\boldsymbol{\theta}}(\mathbf{x}) &= \boldsymbol{\theta}_0 + \boldsymbol{\theta}_1 \mathbf{x}^1 +...+ \boldsymbol{\theta}_m \mathbf{x}^{m}
\\
&= \left[ \boldsymbol{\theta}_0, \boldsymbol{\theta}_1, ..., \boldsymbol{\theta}_m\right] \begin{bmatrix}
1 \\
\mathbf{x}^{1} \\
\vdots \\
\mathbf{x}^{m}
\end{bmatrix}
\\
&=\sum_{i=0}^{m} \boldsymbol{\theta}_i \mathbf{x}^{i},\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
avec comme attribut le vecteur
\(\phi (\mathbf{x}) = \left[ 1, \mathbf{x}^1, ..., \mathbf{x}^m\right]^T\).
Ainsi, deux méthodes existent pour déterminer le meilleur paramètre
\(\boldsymbol{\theta}^*\).
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimation par la méthode du maximum de vraisemblance (appelée MLE:
Maximum Likelihood Estimation)}: Suivant de manière analogique de la
détermination du paramètre \(\boldsymbol{\theta}^*\) sur la
partie précédente, la meilleure valeur du paramètre
\(\boldsymbol{\theta}^{*}\) est déterminée par
\(\boldsymbol{\theta}^*= (\mathbf{x}^T \mathbf{x})^{-1}\mathbf{x}^T\mathbf{y}.\)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimation par la méthode d’un posteriori maximal (appelée MAP:
Maximum A Posteriori)}: La méthode consiste à trouver la valeur
\(\boldsymbol{\theta}^{*}_{\mathrm{MAP}}\) qui maximise le
produit entre la vraisemblance et la distribution à priori des
paramètres \(\boldsymbol{\theta}\) comme l’indique l’équation
{\hyperref[\detokenize{chapter3:naivebayes}]{\emph{{[}naivebayes{]}}}} (\autopageref*{\detokenize{chapter3:naivebayes}}). Cette méthode d’estimation apparaît
généralement dans un cadre bayésien. Tout comme la méthode du maximum
de vraisemblance, elle peut être utilisée afin d’estimer un certain
nombre de paramètres inconnus, comme les paramètres d’une densité de
probabilité, reliés à un échantillon donné. La seule différence avec
la méthode de maximum de vraisemblance est sa possibilité de prendre
en compte un à priori non uniforme sur les paramètres à estimer.
Ainsi, nous pouvons dire que l’estimateur au maximum de vraisemblance
est l’estimateur MAP pour une distribution à priori uniforme. Par le
théorème de Bayes, nous pouvons obtenir le postérieur comme un
produit de vraisemblance avec :
\begin{equation}\label{equation:chapter3:chapter3:16}
\begin{split}\begin{aligned}
        \mathbb{P}\left(\boldsymbol{\theta}|\mathbf{y};\mathbf{x} \right) &= \frac{\mathbb{P}\left(\mathbf{y};\mathbf{x}|\boldsymbol{\theta}\right) \mathbb{P}\left(\boldsymbol{\theta} \right)}{\mathbb{P}\left(\mathbf{y};\mathbf{x}\right)}
        \\
        & \propto \mathbb{P}\left(\mathbf{y};\mathbf{x}|\boldsymbol{\theta}\right)\mathbb{P}\left(\boldsymbol{\theta}\right).\nonumber
    \end{aligned}\end{split}
\end{equation}
\end{enumerate}

\sphinxAtStartPar
Avec
\(\mathbf{Y}| \boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\theta}^T \mathbf{x}, \sigma^2)\)
et
\(\boldsymbol{\theta} \sim \mathcal{N}(\mathbf{0}, \lambda^2 \mathbf{I})\)
où \(\mathbf{I}\) représente la matrice identité dont la dimension
est la longueur du vecteur \(\boldsymbol{\theta}\). Ainsi, nous
pouvons écrire la vraisemblance comme:
\begin{equation}\label{equation:chapter3:chapter3:17}
\begin{split}\mathbb{P}(\boldsymbol{\theta}| \mathbf{y};\mathbf{x}) = \frac{1}{\sigma \sqrt{2\pi }}\exp{\left(-\frac{(y - \boldsymbol{\theta}^T \mathbf{x})^2}{2 \sigma^2}\right)} \frac{1}{\lambda \sqrt{2\pi }} \operatorname{exp}\left(-\frac{\boldsymbol{\theta}^2}{2 \lambda^2}\right)\end{split}
\end{equation}
\sphinxAtStartPar
En utilisant la fonction logarithme, nous avons
\begin{equation}\label{equation:chapter3:chapter3:18}
\begin{split}\begin{aligned}
 \log \mathbb{P}(\boldsymbol{\theta}|\mathbf{x}, \mathbf{y}) &= \log\left(\frac{1}{\sigma \sqrt{2\pi }}\operatorname{exp}\left(-\frac{(\mathbf{y} - \boldsymbol{\theta}^T \mathbf{x})^2}{2 \sigma^2}\right) \frac{1}{\lambda \sqrt{2\pi }}\exp{\left(-\frac{\boldsymbol{\theta}^2}{2 \lambda^2}\right)} \right)\\
 &= -\frac{1}{2 \sigma^2}(\mathbf{y} - \boldsymbol{\theta}^T \mathbf{x})^2 -\frac{1}{2 \lambda^2}\boldsymbol{\theta} ^2 + c^{te}
\\
& = -\frac{1}{2 \sigma^2}||\mathbf{y} - \boldsymbol{\theta} \mathbf{x}||^2 -\frac{1}{2 \lambda^2}||\boldsymbol{\theta}|| ^2 + c^{te}.\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Et le paramètre à estimer \(\boldsymbol{\theta}^*\) correspond au
\(\boldsymbol{\theta}\) qui annule la dérivée partielle de
\(\log \mathbb{P}(\mathbf{y}| \mathbf{x}, \boldsymbol{\theta})\) par
rapport à \(\boldsymbol{\theta}\).

\begin{sphinxuseclass}{center}
\sphinxAtStartPar
\(\displaystyle \frac{\partial \log \mathbb{P}(\mathbf{y}| \mathbf{x}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0  \Longleftrightarrow \frac{\partial \log}{\partial \boldsymbol{\theta}}\left(-\frac{1}{2 \sigma^2}||\mathbf{y} - \boldsymbol{\theta} \mathbf{x}||^2 -\frac{1}{2 \lambda^2}||\boldsymbol{\theta}||^2 + c^{te} \right) = 0\).

\end{sphinxuseclass}
\sphinxAtStartPar
\(\linebreak\) Ceci revient à déterminer le
\(\boldsymbol{\theta}\) qui annule l’expression
\begin{equation}\label{equation:chapter3:chapter3:19}
\begin{split}\begin{aligned}
  \frac{1}{ \sigma^2}\mathbf{x}^T (\mathbf{y} - \boldsymbol{\theta} \mathbf{x}) -\frac{1}{ \lambda^2} \boldsymbol{\theta}. \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Alors,
\begin{equation}\label{equation:chapter3:chapter3:20}
\begin{split}\begin{aligned}
    &-\frac{1}{ \sigma^2}\mathbf{x}^T (\mathbf{y} - \boldsymbol{\theta} \mathbf{x}) -\frac{1}{ \lambda^2} \boldsymbol{\theta} = 0
    \Longleftrightarrow
    -\frac{1}{ \sigma^2}\mathbf{x}^T \mathbf{y} + \frac{1}{ \sigma^2} \boldsymbol{\theta} \mathbf{x}^T \mathbf{X} - \frac{1}{\lambda^2}\boldsymbol{\theta} = 0
    \\
    \\
    &~(\frac{1}{\sigma^2}\mathbf{x}^T \mathbf{x} - \frac{1}{\lambda^2}) \boldsymbol{\theta} = \frac{1}{\sigma^2}\mathbf{x}^T \mathbf{y}
    \Longleftrightarrow
  \boldsymbol{\theta}^* = \left(\mathbf{x}^T \mathbf{x} - \frac{\sigma^2}{\lambda^2} \mathbf{I} \right)^{-1} \mathbf{x}^T\mathbf{y}.\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Cas Pratique


\subsection{Les Problèmes de Classification}
\label{\detokenize{chapter3:les-problemes-de-classification}}
\sphinxAtStartPar
A la différence avec le problème de régression, la classification est un
autre type de problème d’apprentissage supervisé où la variable à
prédire est discrète (ou qualitative ou catégorique). Cette variable
discrète peut être binaire (deux classes) ou multiple (multi\sphinxhyphen{}classe).
Par exemple lorsqu’on veut \sphinxstylestrong{catégoriser} si un e\sphinxhyphen{}mail reçu est un
‘spam’ ou “non\sphinxhyphen{}spam” il s’agit bel et bien d’un problème de
classification.


\subsubsection{L’algorithme des \protect\(K\protect\) plus proches voisins (\protect\(K\protect\)\sphinxhyphen{}NN)}
\label{\detokenize{chapter3:lalgorithme-des-k-plus-proches-voisins-k-nn}}
\sphinxAtStartPar
L’algorithme des \(K\) plus proches voisins aussi appelé
\(K\)\sphinxhyphen{}Nearest Neighbors (\(K\)\sphinxhyphen{}NN) en anglais est une méthode
d’apprentissage supervisé utilisée pour la classification aussi bien que
la régression () {[}@goodfellow2016deep{]}. Il est
compté parmi les plus simples algorithmes d’apprentissage automatique
supervisé, facile à mettre en oeuvre et à comprendre.

\sphinxAtStartPar
Toutefois dans l’industrie, il est plus utilisé pour les problèmes de
classification. Son fonctionnement se base sur le principe suivant: \sphinxstyleemphasis{dis
moi qui sont tes voisins, je te dirais qui tu es …}

\sphinxAtStartPar
L’objectif de cet algorithme est de déterminer la classe d’une nouvelle
observation \(x\) en fonction de la classe majoritaire parmi ses
\(K\) plus proches voisins. Donc l’algorithme est basé sur la mesure
de similarité des voisins proches pour classifier une nouvelle
observation \(x\).

\sphinxAtStartPar
La méthode des \(K\) plus proches voisins, où \(K\) représente
le nombre de voisins proches est une méthode non\sphinxhyphen{}paramétrique. Cela
signifie que l’algorithme permet de faire une classification sans faire
d’hypothèse sur la fonction
\(y=f(\mathbf{x}_1,\mathbf{x}_2, \dots \mathbf{x}_n)\) qui relie la
variable dépendante \(\mathbf{y}\) aux variables indépendantes
\(\mathbf{x}_1,\mathbf{x}_2, \dots, \mathbf{x}_n\).

\sphinxAtStartPar
Soit \(\mathcal{D}\) l’ensemble des données ou l’échantillon
d’apprentissage, défini par:
\begin{equation}\label{equation:chapter3:chapter3:21}
\begin{split}\mathcal{D}=\{(\mathbf{x}_i, y_i), i=1, \dots, n\},\end{split}
\end{equation}
\sphinxAtStartPar
où \(y_i \in \{1,\dots,c\}\) dénote la classe de la donnée
\(i\) et
\(\mathbf{x}_i=(\mathbf{x}_{i1}, \dots, \mathbf{x}_{im})\) est le
vecteur représentant les variables (attributs) prédictrices de la donnée
\(i\).

\sphinxAtStartPar
Supposons un nouveau point \(\textbf{p}\) pour lequel nous voulons
prédire la classe dans la quelle il doit appartenir comme indiqué dans
la figure {\hyperref[\detokenize{chapter3:fig:Knn}]{\emph{1.7}}} (\autopageref*{\detokenize{chapter3:fig:Knn}}).

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{KNN}.png}
\caption{Classification d’un nouveau point entre deux classes}\label{\detokenize{chapter3:id10}}\end{figure}

\sphinxAtStartPar
La première chose à faire est de calculer la distance entre le point
\(\textbf{p}\) avec tous les autres points. Ensuite trouver les
\(K\) points les plus proches de \(\textbf{p}\). C’est\sphinxhyphen{}à\sphinxhyphen{}dire
les \(K\) points dont la distance avec \(\textbf{p}\) est
minimale. Les \(K-\)points plus proches de \(\textbf{p}\) dans
l’échantillon d’apprentissage sont obtenus par:
\begin{equation}\label{equation:chapter3:chapter3:22}
\begin{split}\underset{\mathbf{x}_i}{K-\mbox{argmin}}\  \{d(\textbf{p},\mathbf{x}_i), i=1, \dots, n\}.\end{split}
\end{equation}
\sphinxAtStartPar
Pour tout
\(i \in \{1, \dots, n\}, d_{p, i} := \{ d(p, \mathbf{x}_i), ~ i = 1, \dots, n \}\)
où \(d\) est une fonction de distance. Et en suite la classe prédite
de \(\textbf{p}\) notée \(\hat{\textbf{y}}\) est la classe
majoritairement représentée par les \(k\) voisins.

\sphinxAtStartPar
Les points similaires ou les points les plus proches sont sélectionnés
en utilisant une fonction de distance telle que la distance
euclidienne {\hyperref[\detokenize{chapter3:Euclidienne}]{\emph{{[}Euclidienne{]}}}} (\autopageref*{\detokenize{chapter3:Euclidienne}}), la distance de
Manhattan {\hyperref[\detokenize{chapter3:Manhattan}]{\emph{{[}Manhattan{]}}}} (\autopageref*{\detokenize{chapter3:Manhattan}}) et la distance de Minkowski
 {\hyperref[\detokenize{chapter3:Minkowski}]{\emph{{[}Minkowski{]}}}} (\autopageref*{\detokenize{chapter3:Minkowski}}). On choisit la fonction de distance en
fonction des types de données manipulées, par exemple dans le cas où les
données sont quantitatives et du même type, c’est la distance
euclidienne qui est utilisée.

\sphinxAtStartPar
Les points les plus proches de \(P\) sont trouvés en utilisant une
fonction de distance telle que la distance Euclidienne, la distance de
Minkowski et la distance de Manhattan.


\paragraph{Algorithme}
\label{\detokenize{chapter3:algorithme}}
\sphinxAtStartPar
Soient \(\mathcal{D}\) un échantillon d’apprentissage des
observations \(\mathbf{x}_i\) relatives à une classe \(y_i\)
\(\mathcal{D}=\{(\mathbf{x}_i, y_i), i=1, \dots,n\}\) et
\(\textbf{p}\) une nouvelle observation dont la classe
\(\hat{c}\) doit être prédite. \sphinxstylestrong{Les étapes de l’algorithme:}
Ainsi, l’algorithme se présente comme suit:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Choisir le paramètre \(K\), le nombre de voisins les plus
proches;

\item {} 
\sphinxAtStartPar
Calculer la distance de la nouvelle observation \(\textbf{p}\)
avec tous les autres échantillons selon une fonction de distance
choisie \(d(p, \mathbf{x}_i);\)

\item {} 
\sphinxAtStartPar
Sélectionner les \(K\) plus proches voisins de
\(\textbf{p}\);

\item {} 
\sphinxAtStartPar
Former la collection \(K_c\) des étiquettes des \(K\) plus
proches voisins de \(\textbf{p}\);

\item {} 
\sphinxAtStartPar
Et la classe de \(\textbf{p}\), \(\hat{c}\) est choisie
d’après la majorité des \(K_c\) plus proches voisins,
c’est\sphinxhyphen{}à\sphinxhyphen{}dire
\begin{equation}\label{equation:chapter3:chapter3:23}
\begin{split}\hat{c}=\mbox{Mode}(K_c)\end{split}
\end{equation}
\sphinxAtStartPar
.

\item {} 
\sphinxAtStartPar
Répéter l’étape 2 à 5 pour chaque nouveau point à classifier.

\end{enumerate}

\sphinxAtStartPar
L’algorithme {\hyperref[\detokenize{chapter3:algorithme_knn}]{\emph{{[}algorithme\_knn{]}}}} (\autopageref*{\detokenize{chapter3:algorithme_knn}}) nous présente le
pseudo\sphinxhyphen{}code de la méthode de plus proches voisins.

\begin{sphinxuseclass}{algorithm}
\begin{sphinxuseclass}{algorithmic}
\sphinxAtStartPar
Un ensemble de données
\(\mathcal{D} = \{(\mathbf{x}_i, y_i)\}, i = 1, \dots,n\)
Choisir une fonction de distance \(d\) Choisir un nombre
\(K \in \mathbb{N}^*\) Pour une nouvelle observation
\(\textbf{p}\) dont on veut prédire la classe \(\hat{c}\):
     Calculer la distance \(d(\textbf{p},\mathbf{x}_i)\)
     Retenir les \(K\) observations proches de
\(\textbf{p}\):
\(\underset{\mathbf{x}_i}{K-\arg \min}\  \{d(\textbf{p},\mathbf{x}_i), i=1, \dots, n\}\)
     Prendre les valeurs \(\displaystyle y_{k}\) des \(K\)
observations retenues:           Si on effectue une régression:
\(\hat{c}= \frac{1}{K}\sum_{k=1}^{K} y_k\) (la moyenne ou la
médiane des \(y_k\) retenues)           Si on effectue une
classification : Calculer le mode des \(y_k\) retenues
     Retourner \(\hat{c}\), la valeur qui a été prédite par
\(K\)\sphinxhyphen{}NN pour l’observation \(\textbf{p}\).

\end{sphinxuseclass}
\end{sphinxuseclass}

\paragraph{Comment choisir la valeur de \protect\(K\protect\) ?}
\label{\detokenize{chapter3:comment-choisir-la-valeur-de-k}}\begin{itemize}
\item {} 
\sphinxAtStartPar
En générale, le choix de la valeur de \(K \in \mathbb{N}^{*}\)
dépend du jeu de données. Pour la classification binaire (en deux
classes) par exemple il est préférable de choisir la valeur \(K\)
impaire pour éviter les votes égalitaires. Historiquement, la valeur
optimale de \(K\) pour la plupart de données est choisie entre 3
et 10 {[}@shalev2014understanding{]}.

\item {} 
\sphinxAtStartPar
Une optimale valeur de \(K\) peut être sélectionnée par diverses
techniques heuristiques dont la \sphinxstylestrong{validation\sphinxhyphen{}croisée} (que nous
allons expliquer ci\sphinxhyphen{}dessous).

\item {} 
\sphinxAtStartPar
Notons que, si l’algorithme est utilisé pour la régression, c’est la
moyenne (ou la médiane) des variables \(\mathbf{y}\) des
\(K\) plus proches observations qui sera utilisée pour la
prédiction. Et dans le cas de la classification, c’est le mode des
variables \(\mathbf{y}\) des \(K\) plus proches observations
qui servira pour la prédiction.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Validation\sphinxhyphen{}croisée (Cross\sphinxhyphen{}Validation)}

\sphinxAtStartPar
La Validation\sphinxhyphen{}croisée (\sphinxstyleemphasis{Cross\sphinxhyphen{}validation}) est une méthode très
populaire utilisée pour estimer la performance d’un algorithme. C’est
une méthode statistique souvent utilisée dans des procédures
d’estimation et aussi pour la sélection de modèles {[}@chen2019mehryar{]}.

\sphinxAtStartPar
Son principe est le suivant :
\begin{itemize}
\item {} 
\sphinxAtStartPar
séparer les données en données en deux échantillon (apprentissage et
validation);

\item {} 
\sphinxAtStartPar
construire l’estimateur sur l’échantillon d’apprentissage et utiliser
l’échantillon de validation pour évaluer l’erreur de prédiction;

\item {} 
\sphinxAtStartPar
Répéter plusieurs fois le processus et enfin faire une moyenne des
erreurs de prédiction obtenues.

\end{itemize}

\sphinxAtStartPar
C’est une technique très utilisée pour le choix de meilleurs paramètres
et hyperparamètres d’un modèle, par exemple pour le choix de la
meilleure valeur de \(K\) dans l’algorithme de plus proches voisins
(\(K\)\sphinxhyphen{}NN).

\sphinxAtStartPar
On distingue les variantes suivantes de la technique de
validation\sphinxhyphen{}croisée:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(K\)\sphinxhyphen{}fold cross\sphinxhyphen{}validation : Partitionnement des données en
\(K\) sous\sphinxhyphen{}ensembles. Chaque sous\sphinxhyphen{}ensemble sert à tour de rôle
d’échantillon de validation et le reste de sous\sphinxhyphen{}ensembles
d’échantillon d’apprentissage. En pratique la valeur de \(K\)
varie entre \(5\) et \(10\).

\item {} 
\sphinxAtStartPar
Leave\sphinxhyphen{}one\sphinxhyphen{}out cross\sphinxhyphen{}validation: qui signifie de laisser à tour de
rôle une observation comme échantillon de validation et le reste des
données comme échantillon d’apprentissage. C’est un \(n\)\sphinxhyphen{}fold
validation\sphinxhyphen{}croisée avec \(n\), le nombre total d’observations.

\item {} 
\sphinxAtStartPar
Leave\sphinxhyphen{}\(q\)\sphinxhyphen{}out qui signifie de laisser à tour de rôle \(q\)
observations comme échantillon de validation et le reste des données
comme échantillon d’apprentissage. C’est une
\(\lceil  \frac{n}{q}\rceil\)\sphinxhyphen{}fold validation\sphinxhyphen{}croisée.

\end{itemize}

\sphinxAtStartPar
D’une manière générale, la procédure de partitionnement des données se
présente souvent comme suit:
\begin{equation}\label{equation:chapter3:chapter3:24}
\begin{split}\begin{aligned}
    \underbrace{Apprentissage }_{70\%} +
    \underbrace{Validation}_{10\%}+
    \underbrace{Test}_{20\%}\end{aligned}\end{split}
\end{equation}\begin{itemize}
\item {} 
\sphinxAtStartPar
Les données d’apprentissage permettent de trouver un estimateur.

\item {} 
\sphinxAtStartPar
Les données de validation nous permettent de trouver les meilleurs
paramètres du modèle.

\item {} 
\sphinxAtStartPar
Les données test permettent de calculer l’erreur de prédiction
finale. Notons que cette méthode de validation\sphinxhyphen{}croisée est utilisée
pour tous types d’algorithme d’apprentissage.

\end{itemize}


\paragraph{Les avantages de l’algorithme de \protect\(K\protect\)\sphinxhyphen{}NN}
\label{\detokenize{chapter3:les-avantages-de-lalgorithme-de-k-nn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Il est simple, facile à interpréter.

\item {} 
\sphinxAtStartPar
Il n’existe pas de phase d’apprentissage proprement dite comme c’est
le cas pour les autres algorithmes, c’est pour cela qu’on le
classifie dans le \sphinxstyleemphasis{Lazy Learning.}

\item {} 
\sphinxAtStartPar
Offre des performances très intéressantes lorsque le volume de
données d’apprentissage est trop large.

\item {} 
\sphinxAtStartPar
Le temps d’exécution est minimum par rapport à d’autres algorithmes
de classification.

\item {} 
\sphinxAtStartPar
Il peut être utilisé pour classification et régression.

\item {} 
\sphinxAtStartPar
Il ne fait pas d’hypothèse (linéaire, affines,..) sur les données.

\end{itemize}


\paragraph{Les limitations de l’algorithme de \protect\(K\protect\)\sphinxhyphen{}NN}
\label{\detokenize{chapter3:les-limitations-de-lalgorithme-de-k-nn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
L’algorithme a plus besoin de mémoire car l’ensemble des données
doivent être garder dans cette dernière pour pouvoir effectuer la
prédiction d’une nouvelle observation à chaque fois.

\item {} 
\sphinxAtStartPar
Sensibles aux attributs non pertinents et non corrélés.

\item {} 
\sphinxAtStartPar
L’étape de la prédiction peur être lente dû au calcul de distance de
chaque nouvelle observation avec les jeux de données en entier à
chaque prédiction.

\item {} 
\sphinxAtStartPar
Le choix de la fonction de distance ainsi que le nombre de voisins
\(K\) peut ne pas être évident. C’est ainsi qu’il faut essayer
plusieurs combinaisons(en utilisant la méthode de validation\sphinxhyphen{}croisée)
pour avoir un bon résultat.

\end{itemize}


\paragraph{Exemple pratique}
\label{\detokenize{chapter3:exemple-pratique}}
\sphinxAtStartPar
Ci\sphinxhyphen{}dessous, nous allons prendre un exemple simple pour comprendre
l’intuition derrière l’algorithme de \(K\)\sphinxhyphen{}NN. Considérons le
tableau de données ci\sphinxhyphen{}dessous qui contient la taille (feet), l’âge
(année) et le poids(en kilogramme) de \(10\) personnes où ID
représente l’identifiant de chaque personne dans le tableau. Comme vous
le remarquez le poids du ID \(11\) est manquant. Nous allons
appliquer l’algorithme de \(K\)\sphinxhyphen{}NN pour prédire le poids de la
personne ID 11.


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Données pour l’illustration}\label{\detokenize{chapter3:id11}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{ID}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Taille}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Âge}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Poids}
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1
&
\sphinxAtStartPar
5
&
\sphinxAtStartPar
45
&
\sphinxAtStartPar
77
\\
\sphinxhline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
5.11
&
\sphinxAtStartPar
26
&
\sphinxAtStartPar
47
\\
\sphinxhline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
5.6
&
\sphinxAtStartPar
30
&
\sphinxAtStartPar
55
\\
\sphinxhline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
5.9
&
\sphinxAtStartPar
34
&
\sphinxAtStartPar
59
\\
\sphinxhline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
4.8
&
\sphinxAtStartPar
40
&
\sphinxAtStartPar
72
\\
\sphinxhline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
5.8
&
\sphinxAtStartPar
36
&
\sphinxAtStartPar
60
\\
\sphinxhline
\sphinxAtStartPar
7
&
\sphinxAtStartPar
5.3
&
\sphinxAtStartPar
19
&
\sphinxAtStartPar
40
\\
\sphinxhline
\sphinxAtStartPar
8
&
\sphinxAtStartPar
5.8
&
\sphinxAtStartPar
28
&
\sphinxAtStartPar
60
\\
\sphinxhline
\sphinxAtStartPar
9
&
\sphinxAtStartPar
5.5
&
\sphinxAtStartPar
23
&
\sphinxAtStartPar
45
\\
\sphinxhline
\sphinxAtStartPar
10
&
\sphinxAtStartPar
5.6
&
\sphinxAtStartPar
32
&
\sphinxAtStartPar
58
\\
\sphinxhline
\sphinxAtStartPar
11
&
\sphinxAtStartPar
5.5
&
\sphinxAtStartPar
38
&
\sphinxAtStartPar
?
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
{[}tab:rrr{]}

\sphinxAtStartPar
Nous pouvons représenter graphiquement les données du tableau
{\hyperref[\detokenize{chapter3:tab:rrr}]{\emph{1.1}}} (\autopageref*{\detokenize{chapter3:tab:rrr}}) en se basant sur la taille et l’âge.

\begin{figure}[H]
\centering

\noindent\sphinxincludegraphics{{KNN_new}.png}
\end{figure}

\sphinxAtStartPar
Comme nous le remarquons, le point rouge (ID 11) est notre nouvelle
observation dont nous voulons prédire la classe dans laquelle il
appartient.

\sphinxAtStartPar
\sphinxstylestrong{Étape 1:} Commençons par choisir le nombre des voisins les plus
proche. Pour notre cas prenons \(K=5\).

\sphinxAtStartPar
\sphinxstylestrong{Étape 2:} Calculer la distance entre le nouveau point (rouge) avec
tous les autres points

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{KNN_dist}.png}
\caption{La distance euclidienne entre nouvelle observation et tous les autres
points}\label{\detokenize{chapter3:id12}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{Étape 3:} Sélectionner les \(5\) plus proches voisins.

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{KNN_sel}.png}
\caption{Sélection de \(5\) plus proches voisins}\label{\detokenize{chapter3:id13}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{Étape 4:} Pour la valeur de \(K=5\), les points les plus proches
sont \(1,\ 4,\ 5,\ 6\text{ et } 10.\)

\sphinxAtStartPar
\sphinxstylestrong{Étape 5:} Ainsi, comme la classe des points \(4,\ 6\) et
\(10\) est majoritaire donc le point \(11\) se classifie dans
cette classe. Et comme c’est un cas de la régression, la prédiction pour
ID11 est la moyenne de ces 5 voisins les plus proches c’est\sphinxhyphen{}à\sphinxhyphen{}dire
\((77+59+72+60+58)/5=65.2\ Kg\). Ainsi le poids prédit pour ID11 est
\(65.2\ Kg\).


\subsubsection{L’algorithme du Perceptron}
\label{\detokenize{chapter3:lalgorithme-du-perceptron}}
\sphinxAtStartPar
L’algorithme de perceptron est un algorithme d’apprentissage supervisé
utilisé pour la classification binaire (c’est\sphinxhyphen{}à\sphinxhyphen{}dire séparant deux
classes). C’est l’un des tout premier algorithme d’apprentissage
supervisé et de réseau de neurones artificiels le plus simple. Le terme
vient de l’unité de base dans un
\sphinxhref{https://fr.wikipedia.org/wiki/R\%C3\%A9seau\_de\_neurones\_artificiels\#Perceptron\_2}{neurone}%
\begin{footnote}[10]\sphinxAtStartFootnote
\sphinxnolinkurl{https://fr.wikipedia.org/wiki/R\%C3\%A9seau\_de\_neurones\_artificiels\#Perceptron\_2}
%
\end{footnote}
qui s’appelle \sphinxstyleemphasis{perceptron}.

\sphinxAtStartPar
C’est un type de classification linéaire, c’est\sphinxhyphen{}à\sphinxhyphen{}dire que les données
d’apprentissage sont séparées par une droite classées dans des
catégories correspondantes de telle sorte que si la classification à
deux catégories est appliquée, toutes les données sont rangées dans ces
deux catégories. Dans ce cas on cherche à trouver un hyperplan qui
sépare correctement les données en deux catégories. Comme nous le voyons
dans la figure {\hyperref[\detokenize{chapter3:fig:perceptron}]{\emph{1.12}}} (\autopageref*{\detokenize{chapter3:fig:perceptron}}) ci\sphinxhyphen{}dessous.

\sphinxAtStartPar
{\color{red}\bfseries{}|L’algorithme du perceptron|} {\color{red}\bfseries{}|L’algorithme du perceptron|}

\sphinxAtStartPar
L’idée générale de l’algorithme du perceptron est d’initialiser le
vecteur de poids réels \(\mathbf{w} \in \mathbb{R}^d\) au vecteur
nul ou à une variable aléatoire, itérer un nombre de fois jusqu’à la
convergence sur les données d’apprentissage.

\sphinxAtStartPar
Soient \(\mathcal{D} = \{(\mathbf{x}_i, y_i)\}^{n}_{i=1}\), un
ensemble de données où \(\mathbf{x}_i \in \mathbb{R}^d\) est le
vecteur d’entrées de dimension \(d\) et l’étiquette
\(y_i \in \{-1,1\}\); \(\mathbf{w} \in \mathbb{R}^d\) un vecteur
poids de dimension \(d\).

\sphinxAtStartPar
L’objectif est de trouver un hyperplan
\(\mathbf{w}.\mathbf{x} + b=0\) qui sépare les deux classes. Le
terme \(b\) est l’intercepte appelé aussi “\sphinxstyleemphasis{bias term}” et
\(\mathbf{w}\cdot \mathbf{x}\) est le produit scalaire défini par
\(\langle \mathbf{w},\mathbf{x} \rangle := \sum_{s=1}^{d} w_{s} \mathbf{x}_{s}\).

\sphinxAtStartPar
C’est\sphinxhyphen{}à\sphinxhyphen{}dire apprendre le vecteur \(\mathbf{w}\) tel que:
\begin{equation}\label{equation:chapter3:chapter3:25}
\begin{split}\begin{aligned}
  \mathbf{y}& = \operatorname{signe}\left(\mathbf{w}\cdot \mathbf{x} +b\right)
  \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar

\begin{equation}\label{equation:chapter3:chapter3:26}
\begin{split}\begin{aligned}
  \text{Si } \mathbf{w}\cdot \mathbf{x} +b>0 \text{ alors } \operatorname{sgn}(\mathbf{w}\cdot \mathbf{x} +b) =+1, \text{ pour tout } \mathbf{x} \text{ appartenant \`a la classe positive.}\\
  \text{Si } \mathbf{w}\cdot \mathbf{x} + b \leq 0 \text{ alors } \operatorname{sgn}(\mathbf{w}\cdot \mathbf{x} +b) = -1, \text{ pour tout } \mathbf{x} \text{ appartenant \`a la classe négative.}
  \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
\#\#\#\# Algorithme

\sphinxAtStartPar
Soient les données d’apprentissage
\(\mathcal{D} = \{(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_n, y_n)\}\)
où \(\mathbf{x}_i \in \mathbb{R}^d\), \(y_i \in \{-1,1\}\) et
\(T\) le nombre d’itérations. L’algorithme se présente comme suit:

\begin{sphinxuseclass}{algorithm}
\begin{sphinxuseclass}{algorithmic}
\sphinxAtStartPar
Initialiser le vecteur \(\mathbf{w} \leftarrow 0\) \sphinxstylestrong{Pour}
itération de 1 à T: **   Pour** chaque exemple
\((\mathbf{x}_i, y_i) \in \mathcal{D}:\)       Calculer la
prédiction
\(\hat{y_i} = \operatorname{signe }( \mathbf{w}. \mathbf{x}_i +b )\)
**      Si** \(\hat{y_i} \neq y_i\) \sphinxstylestrong{alors}          Ajuster
\(\mathbf{w} :\) par
         \(\mathbf{w_{t+1}} \leftarrow \mathbf{w_t}+\mathbf{x}_i\)
si \(y_i\) est positive
         \(\mathbf{w_{t+1}} \leftarrow \mathbf{w_t}-\mathbf{x}_i\)
si \(y_i\) est négative       \sphinxstylestrong{Fin si}    \sphinxstylestrong{Fin pour} \sphinxstylestrong{Fin
pour}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
L’avantage de l’algorithme de perceptron est sa simplicité et son
efficacité de séparer linéairement les données d’apprentissage.
Néanmoins, tous les hyperplans qui séparent les données sont
équivalents.

\sphinxAtStartPar
L’algorithme ne peut séparer et converger vers la solution uniquement
que si les données d’apprentissage sont linéairement séparables. Aussi,
il n’est pas efficace quand il y a beaucoup d’attributs.


\subsubsection{La Régression Logistique}
\label{\detokenize{chapter3:la-regression-logistique}}
\sphinxAtStartPar
La régression logistique est une méthode de classification simple mais
puissante, pour les données binaires, et elle peut facilement être
étendue à plusieurs classes. Considérons d’abord le modèle de régression
le plus simple, correspondant à celui que nous avons vu précédemment,
pour effectuer la classification. Nous le trouverons bientôt totalement
insuffisant pour ce que nous voulons réaliser et il sera instructif de
voir exactement pourquoi. Le modèle de la régression linéaire comme
défini dans l’équation {\hyperref[\detokenize{chapter3:reg_lin}]{\emph{{[}reg\_lin{]}}}} (\autopageref*{\detokenize{chapter3:reg_lin}}), peut se réécrire comme:
\begin{equation}\label{equation:chapter3:chapter3:27}
\begin{split}h_{\boldsymbol{\theta}}(\mathbf{X})=  \mathbf{X}\boldsymbol{\theta},\end{split}
\end{equation}
\sphinxAtStartPar
où \(\mathbf{X}\) est une matrice de taille \(n\times d\) et
\(\boldsymbol{\theta} \in \mathbb{R}^{d}\).

\sphinxAtStartPar
Comme dans de nombreux problèmes d’estimation de paramètres,
\(\boldsymbol{\theta}\) est trouvé en minimisant certaines fonctions
de pertes qui capturent à quel point notre prédiction est proche de la
valeur réelle. Quand nous faisons des hypothèses sur la distribution des
données, la fonction de perte est souvent en termes de vraisemblance des
données. Cela signifie que le \(\boldsymbol{\theta}\) optimal est
celui pour lequel les données observées ont la probabilité la plus
élevée. Par exemple, la régression linéaire suppose généralement que la
variable dépendante est normalement distribuée autour de la moyenne
\(h_{\boldsymbol{\theta}}(\mathbf{x})\). Il peut être montré que la
solution du maximum de vraisemblance est le \(\boldsymbol{\theta}\)
qui minimise la somme des erreurs quadratiques (la différence entre les
valeurs prédites et les valeurs correctes). Si nous essayons d’utiliser
la régression linéaire pour un problème de classification (prenons
l’exemple d’une classification binaire) une méthode simple serait de
grouper les données de telle sorte que:
\begin{equation}\label{equation:chapter3:chapter3:28}
\begin{split}y=
\begin{cases}
1,& \text{si } \boldsymbol{\theta}^{T}\mathbf{x} > 0 \quad (\mathbf{x}\in \mathbb{R}^{d}) \\
0,& \text{sinon}
\end{cases}\end{split}
\end{equation}
\sphinxAtStartPar
Ceci est illustré par la figure {\hyperref[\detokenize{chapter3:Figure1}]{\emph{1.13}}} (\autopageref*{\detokenize{chapter3:Figure1}}).

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{linear_for_classi}.png}
\caption{Régression linéaire dans le cas d’une classification binaire}\label{\detokenize{chapter3:id14}}\end{figure}


\paragraph{La régression logistique binaire}
\label{\detokenize{chapter3:la-regression-logistique-binaire}}
\sphinxAtStartPar
La régression logistique ordinaire ou régression logistique binaire vise
à expliquer une variable d’intérêt binaire (c’est\sphinxhyphen{}à\sphinxhyphen{}dire de type « oui /
non » ou « vrai / faux »). Les variables explicatives qui seront
introduites dans le modèle peuvent être quantitatives (l’âge, la taille,
etc) ou qualitatives (le genre par exemple).

\sphinxAtStartPar
\sphinxstylestrong{Exemple}

\sphinxAtStartPar
Dans cet exemple, la variable explicative \(x\) est une matrice de
vecteurs colonnes \(\mathbf{x}_1, \mathbf{x}_2\) où
\(\mathbf{x}_1\) est le vecteur ‘apprendre’ qui représente le nombre
d’heures d’étude de l’étudiant, et \(\mathbf{x}_2\) est le nombre
d’heures pendant lesquelles l’étudiant dort. L’objectif est de prédire
si l’étudiant va réussir à l’examen ou non, respectivement représentée
par les classes 1 et 0.

\begin{sphinxuseclass}{center}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Apprendre
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Dormir
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Réussir
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
4.85
&
\sphinxAtStartPar
9.63
&
\sphinxAtStartPar
1
\\
\sphinxhline
\sphinxAtStartPar
8.62
&
\sphinxAtStartPar
3.23
&
\sphinxAtStartPar
0
\\
\sphinxhline
\sphinxAtStartPar
5.43
&
\sphinxAtStartPar
8.23
&
\sphinxAtStartPar
1
\\
\sphinxhline
\sphinxAtStartPar
9.21
&
\sphinxAtStartPar
6.34
&
\sphinxAtStartPar
0
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\end{sphinxuseclass}
\sphinxAtStartPar
Comme dans le cas de la régression linéaire, on suppose que les données
suivent une fonction linéaire de la forme:
\begin{equation}\label{equation:chapter3:chapter3:29}
\begin{split}\mathbf{y}= \boldsymbol{\theta}^T\mathbf{x}.\end{split}
\end{equation}
\sphinxAtStartPar
\(\mathbf{y}\) représente la variable à expliquer,
\(\mathbf{x}\) est la variable explicative et
\(\boldsymbol{\theta}\) un paramètre. Comme nous l’avons vu, la
variable \(\mathbf{y}\) est une variable continue. Pour utiliser
cette technique dans le cas d’une variable discrète (ici binaire),
supposons que \(p\) est la probabilité qu’un événement se réalise;
alors \(1-p\) est la probabilité de l’évènement contraire. La
variable aléatoire \(\mathbf{y}\) qui prend les valeurs ‘oui’ ou
‘non’ (1,0 en langage machine) suit la loi de Bernoulli. On définit ce
qu’on appelle la transformation logit donnée par l’équation suivante:
\begin{equation}\label{equation:chapter3:chapter3:30}
\begin{split}\log\left(\frac{p}{1-p}\right)=\boldsymbol{\theta}^T\mathbf{x} .\end{split}
\end{equation}
\sphinxAtStartPar
Cette transformation donne la relation entre la probabilité qu’un
évenment se réalise et la combinaison linéaire des variables. Le rapport
\(\frac{p}{1-p}\) est appelé le Rapport de Côte (RC). En appliquant
l’exponentielle sur cette relation on obtient:
\begin{equation}\label{equation:chapter3:chapter3:31}
\begin{split}\frac{p}{1-p} =  e^{\boldsymbol{\theta}^T\mathbf{x}}.\end{split}
\end{equation}
\sphinxAtStartPar
Ce qui implique:
\begin{equation}\label{equation:chapter3:chapter3:32}
\begin{split}\begin{aligned}
p &=\frac{e^{\boldsymbol{\theta}^T\mathbf{x}}}{1+e^{\boldsymbol{\theta}^T\mathbf{x}}}\nonumber\\
%&=\frac{1}{\frac{1}{e^{X\boldsymbol{\theta}+1}}}\nonumber\\
&=\frac{1}{1+e^{-\boldsymbol{\theta}^T\mathbf{x}}}\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Avec \(z= \boldsymbol{\theta}^T\mathbf{x}\), la fonction
\(\sigma\) définie par
\(\displaystyle \sigma(z)=\frac{1}{1+e^{-z}}\) est appelée la
fonction Sigmoïde ou logistique. Dans les lignes qui suivent, nous
allons donner certaines de ses propriétés.


\subsubsection{La fonction Sigmoïde et ses propriétés}
\label{\detokenize{chapter3:la-fonction-sigmoide-et-ses-proprietes}}
\sphinxAtStartPar
Elle est définie par:
\begin{equation}\label{equation:chapter3:chapter3:33}
\begin{split}\sigma(z)=\dfrac{1}{1+e^{-z}},\end{split}
\end{equation}
\sphinxAtStartPar
La représentation graphique de la fonction Sigmoïde est donnée par la
figure {\hyperref[\detokenize{chapter3:sigmoid}]{\emph{1.14}}} (\autopageref*{\detokenize{chapter3:sigmoid}}).

\begin{figure}[H]
\centering
\capstart

\noindent\sphinxincludegraphics{{logistic2}.png}
\caption{La fonction Sigmoïde}\label{\detokenize{chapter3:id15}}\end{figure}

\sphinxAtStartPar
Une qualité importante de cette fonction est qu’elle transforme tous les
nombres réels sur la plage \([0, 1]\). En régression logistique,
cette transformation \(\sigma (z)\) nous permet d’avoir une vue
probabiliste qui est d’une importance cruciale pour la classification.
Avec cette fonction, les nombres positifs deviennent des probabilités
élevées; les nombres négatifs deviennent de faible probabilité.

\sphinxAtStartPar
Comme vous l’avez sûrement remarqué, l’algorithme de régression linéaire
repose sur l’obtention d’un paramètre \(\boldsymbol{\theta}^{*}\)
dit optimal. Dans la section suivante nous allons discuter sur le choix
et l’obtention de la valeur \(\boldsymbol{\theta}^{*}\).


\paragraph{Estimation du maximum de vraisemblance}
\label{\detokenize{chapter3:estimation-du-maximum-de-vraisemblance}}
\sphinxAtStartPar
Pour choisir la valeur du paramètre \(\boldsymbol{\theta}\), on
utilise la méthode du maximum de vraisemblance. La variable à expliquer
\(\mathbf{y}\) est une variable binaire, i.e
\(\mathbf{y}\in \{0,1\}\) et la fonction Sigmoïde nous permet de
projeter les résultats dans l’intervalle \([0, 1]\). Plus
précisément, pour un \(z\) considéré, on choisit
\(\sigma (z) \in [0,1]\) comme étant un paramètre d’une loi de
Bernoulli et ainsi, on a pour tout \(i=1, \dots, n\):
\begin{equation}\label{equation:chapter3:chapter3:34}
\begin{split}\begin{aligned}
\mathbb{P}(Y=y_{i})&=\left(\sigma(z)\right)^{y_{i}}\left(1-\sigma(z)\right)^{1-y_{i}}\\
&=\left(\sigma(\boldsymbol{\theta}^T\mathbf{x}_i)\right)^{y_{i}} \left (1-\sigma(\boldsymbol{\theta}^T\mathbf{x}_i)\right)^{1-y_{i}}\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Par suite, on peut exprimer la vraisemblance:
\begin{equation}\label{equation:chapter3:chapter3:35}
\begin{split}\begin{aligned}
\ell(\boldsymbol{\theta})&=\prod_  {i=1}^{n}\mathbb{P}\left(Y=y_{i}\right)\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
En appliquant la fonction logarithme, on obtient le
logarithme\sphinxhyphen{}vraisemblance donnée par l’expression suivante:
\begin{equation}\label{equation:chapter3:chapter3:36}
\begin{split}\begin{aligned}
    \log( \ell(\boldsymbol{\theta}))\equiv L(\boldsymbol{\theta})=\sum_{i=1}^{n}y_{i}\log\sigma\left(\boldsymbol{\theta}^{T}\mathbf{x}_i\right) + \left(1-y_{i}\right)\log\left(1-\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i)\right)\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Notre objective est de trouver le paramètre \(\boldsymbol{\theta}\)
qui maximise la vraisemblance:
\begin{equation}\label{equation:chapter3:chapter3:37}
\begin{split}\begin{aligned}
 \max_{\boldsymbol{\theta}}
 L(\boldsymbol{\theta}) = \max_{\boldsymbol{\theta}}\sum_{i=1}^{n}y_{i}\log\sigma \left(\boldsymbol{\theta}^{T}\mathbf{x}_i \right) + \left(1-y_{i}\right)\log \left(1-\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i)\right)   \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
En générale, pour trouver l’estimation du maximum de vraisemblance, nous
dérivons d’abord la \(\log\)\sphinxhyphen{}vraisemblance par rapport à
\(\boldsymbol{\theta}\). Pour commencer, prenons la dérivée par
rapport à une composante de \(\boldsymbol{\theta}\), disons
\(\theta^j\)
\begin{equation}\label{equation:chapter3:chapter3:38}
\begin{split}\begin{aligned}
\frac{\partial}{\partial \theta^j}L(\boldsymbol{\theta})
&=\left[ y_i\frac{1}{\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i)}-(1-y_i)\frac{1}{1-\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i)}\right]\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i)(1-\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i))(\frac{\partial}{\partial \theta^j}\boldsymbol{\theta}^{T}\mathbf{x}_i)\\
&=\left[y_i(1-\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i))-(1-y_{i})\sigma(\boldsymbol{\theta}^{T}\mathbf{x}_i)\right]\mathbf{x}_i^{j}\\
&=\left(y_{i}-\sigma(\boldsymbol{\theta}^{T} \mathbf{x}_i)\right)\mathbf{x}_i^{j}\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Une méthode classique de trouver le \(\boldsymbol{\theta}\) optimal
est de poser la dérivée
\(\displaystyle \frac{\partial}{\partial \theta^j}L(\boldsymbol{\theta})=0\)
pour tout \(j\) et trouver la valeur exacte de
\(\boldsymbol{\theta}\) qui maximise la vraisemblance. Cependant, la
solution exacte n’est pas toujours facile à calculer à cause du fait que
c’est une équation transcendantale (il n’y a pas de solution
analytique). Par conséquent, la technique souvent utilisée pour résoudre
ce problème est la méthode de la descente de gradient {\hyperref[\detokenize{chapter3:GD}]{\emph{1.1.1.2}}} (\autopageref*{\detokenize{chapter3:GD}}).

\sphinxAtStartPar
Ainsi en utilisant la technique de la descente de gradient, le paramètre
\(\theta^j\) sera mis à jour par la formule suivante, pour chaque
itération:
\begin{equation}\label{equation:chapter3:chapter3:39}
\begin{split}\theta^j = \theta^j + \gamma . \frac{\partial}{\partial \theta^j}L(\boldsymbol{\theta}).\end{split}
\end{equation}
\sphinxAtStartPar
Le paramètre \(\gamma\) est appelé taux d’apprentissage. Le
paramètre \(\boldsymbol{\theta}^{*}\) obtenu à la sortie de cet
algorithme maximise la log\sphinxhyphen{}vraisemblance.

\sphinxAtStartPar
Ainsi nous considérons un seuil 0.5 et regroupons les données comme
suit:
\begin{equation}\label{equation:chapter3:chapter3:40}
\begin{split}\hat{y}=
\begin{cases}
1,& \text{si } \sigma(\boldsymbol{\theta}^{*T}\mathbf{x}) > 0.5 \\
0,& \text{sinon}
\end{cases}\end{split}
\end{equation}

\paragraph{Cas pratique}
\label{\detokenize{chapter3:cas-pratique}}

\subsubsection{Régression logistique multinomiale}
\label{\detokenize{chapter3:regression-logistique-multinomiale}}
\sphinxAtStartPar
La régression logistique multinomiale est une forme généralisée de la
régression logistique binaire utilisée pour estimer la probabilité quand
le nombre de classe \(C\) est supérieur à deux. Prenons l’exemple de
reconnaissance de chiffres à partir de leur image. Cette tâche consiste
à identifier une image comme l’un des éléments de l’ensemble
\(\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}\). Dans ce cas, la fonction
{[}\textasciicircum{}1{]}Sigmoïde utilisée dans le cas binaire est remplacée par la
{[}\textasciicircum{}2{]}\sphinxstyleemphasis{fonction Softmax}.


\subsubsection{La fonction Softmax et ses propriétés}
\label{\detokenize{chapter3:la-fonction-softmax-et-ses-proprietes}}\label{\detokenize{chapter3:estimation-du-maximum-de-vraisemblance-1}}
\sphinxAtStartPar
La régression logistique multinomiale utilise une \sphinxstyleemphasis{fonction Softmax}
pour modéliser la relation entre les variables explicatives et les
probabilités de chaque classe. Elle prédit la classe qui a la
probabilité la plus élevée parmi toutes les classes.

\sphinxAtStartPar
La fonction \(\operatorname{Softmax}\) est définie par:
\begin{equation}\label{equation:chapter3:chapter3:41}
\begin{split}\operatorname{Softmax}(z)_i=\dfrac{e^{z_i}} {\sum_{j=1}^{C} e^{z_{j}}}, \quad i=1,\cdots, C.
u\end{split}
\end{equation}
\sphinxAtStartPar
où \(C\) est le nombre de classes.

\sphinxAtStartPar
Cette fonction \(\operatorname{Softmax}\) prend comme entrée
\(\mathbf{z}\) qui est un vecteur de dimension \(n\) et produit
\(\mathbf{\hat{y}}\) un vecteur de même dimension de valeurs réelles
entre \(0\) et \(1\).

\sphinxAtStartPar
Toutes les valeurs \(z_{i}\) sont les éléments du vecteur d’entrée
et peuvent prendre n’importe quelle valeur réelle. Le dénominateur de la
formule {\hyperref[\detokenize{chapter3:soft}]{\emph{{[}soft{]}}}} (\autopageref*{\detokenize{chapter3:soft}}) est le terme de normalisation qui garantit
que toutes les valeurs de sortie de la fonction totaliseront \(1\),
constituant ainsi une distribution de probabilité valide.

\sphinxAtStartPar
On peut écrire la probabilité de la classe \(c\) pour
\(c=1,\cdots,C\) sachant \(\mathbf{x}\) comme:
\begin{equation}\label{equation:chapter3:chapter3:42}
\begin{split}\begin{aligned}
\begin{bmatrix}
\mathbb{P}(y=1|\mathbf{x}) \\
\vdots \\
\mathbb{P}(y=C|\mathbf{x})
\end{bmatrix} =
\begin{bmatrix}
\operatorname{Softmax}(\mathbf{x})_{1} \\
\vdots \\
\operatorname{Softmax}(\mathbf{x})_{C}
\end{bmatrix}=
\dfrac{1}{\sum_{j=1}^{C}e^{\boldsymbol{\theta}^T \mathbf{x}_{j}}}
\begin{bmatrix}
e^{\boldsymbol{\theta}^T \mathbf{x}_1}\\
\vdots \\
e^{\boldsymbol{\theta}^T \mathbf{x}_{C}}
\end{bmatrix}\end{aligned}\end{split}
\end{equation}

\subsubsection{Estimation du maximum de vraisemblance}
\label{\detokenize{chapter3:id2}}
\sphinxAtStartPar
De la même façon que dans le cas de la régression binaire, nous suivrons
la même procédure pour déterminer le \(\boldsymbol{\theta}\) qui
maximise la vraisemblance.
\begin{equation}\label{equation:chapter3:chapter3:43}
\begin{split}\ell(\boldsymbol{\theta})= \prod_{i=1}^{n}\mathbb{P}(y_{i}|\mathbf{x};\boldsymbol{\theta}).\end{split}
\end{equation}
\sphinxAtStartPar
L’équation {\hyperref[\detokenize{chapter3:lv}]{\emph{{[}lv{]}}}} (\autopageref*{\detokenize{chapter3:lv}}) suppose que les instances de données ont été
générées indépendamment. Ainsi, en appliquant le logarithme
sur {\hyperref[\detokenize{chapter3:lv}]{\emph{{[}lv{]}}}} (\autopageref*{\detokenize{chapter3:lv}}), nous obtenons:
\begin{equation}\label{equation:chapter3:chapter3:44}
\begin{split}\begin{aligned}
    L(\boldsymbol{\theta})&=\sum_{j=1}^{n}\sum_{i=1}^{C} \log\left[\left(\dfrac{e^{\boldsymbol{\theta}_{i}^T \mathbf{x}_{j}}} {\sum_{k=1}^{C} e^{\boldsymbol{\theta}_{k}^T \mathbf{x}_{j}}}\right)^{y_{j}}\right]\\
    &=\sum_{j=1}^{n}\sum_{i=1}^{C}y_{j}\log\left(\dfrac{e^{\boldsymbol{\theta}^{T}_i \mathbf{x}_{j}}} {\displaystyle\sum_{k=1}^{C} e^{\boldsymbol{\theta}^{T}_i \mathbf{x}_{j}}}\right).\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Donc maximiser la \(\operatorname{log-vraisemblance}\) équivaut à
minimiser l’opposé de la \(\operatorname{log-vraisemblance}\) donnée
par l’expression suivante :
\begin{equation}\label{equation:chapter3:chapter3:45}
\begin{split}L(\boldsymbol{\theta})= -\sum_{j=1}^{n}\sum_{i=1}^{C}y_{j}\log\left(\dfrac{e^{\boldsymbol{\theta}_{i}^T \mathbf{x}_{j}}} {\sum_{k=1}^{C} e^{\boldsymbol{\theta}^{T}_k \mathbf{x}_{j}}}\right)
  =- \sum_{j=1}^{n}\sum_{i=1}^{C}y_{j}\left[\log\left(e^{\boldsymbol{\theta}_{i}^T \mathbf{x}_{j}} \right) - \log\left( \displaystyle\sum_{k=1}^{C} e^{\boldsymbol{\theta}_{k}^T \mathbf{x}_{j}} \right)\right].\end{split}
\end{equation}
\sphinxAtStartPar
Cet opposé de la \(\operatorname{log-vraisemblance}\) est aussi
connu sous le nom de \sphinxstylestrong{l’entropie de Shannon (cross\sphinxhyphen{}entropy)} qui est
la fonction de perte dans ce cas. Pour trouver le
\(\boldsymbol{\theta}\) qui minimise cette fonction de perte, on
suit la même procédure que dans le cas de la régression logistique,
c’est\sphinxhyphen{}à\sphinxhyphen{}dire trouver la dérivée de la fonction de perte et appliquer
l’algorithme de la descente de gradient.

\sphinxAtStartPar
En Calculant la dérivée de la fonction de perte par rapport à
\(\boldsymbol{\theta}_{j}\) on a:
\begin{equation}\label{equation:chapter3:chapter3:46}
\begin{split}\frac{\partial}{\partial \boldsymbol{\theta}_{j}}L(\boldsymbol{\theta})=-\sum_{i=1}^{n}y_{i}\mathbf{x}_i + \sum_{i=1}^{n}
\dfrac{1}{{\sum_{k=1}^{C}e^{\boldsymbol{\theta}_{k}^T \mathbf{x}_i}}}e^{\boldsymbol{\theta}_{j}^T\mathbf{x}_i}\mathbf{x}_i\end{split}
\end{equation}
\sphinxAtStartPar
Maintenant on utilise l’algorithme de la descente de gradient durant le
processus d’entraînement pour obtenir le \(\boldsymbol{\theta}\)
optimal. À chaque itération, on met à jour chacun des paramètres
\(\boldsymbol{\theta}_j\) par la formule suivante:
\begin{equation}\label{equation:chapter3:chapter3:47}
\begin{split}\boldsymbol{\theta}_{j} = \boldsymbol{\theta}_{j} - \gamma\frac{\partial}{\partial \boldsymbol{\theta}_{j}}L(\boldsymbol{\theta}).\end{split}
\end{equation}
\sphinxAtStartPar
A la sortie de cette algorithme nous obtenons le paramètre optimal
\(\boldsymbol{\theta}^{*}\). Ainsi, le vecteur
\(\mathbf{\hat{y}}\) prédit est donné par:
\begin{equation}\label{equation:chapter3:chapter3:48}
\begin{split}\mathbf{\hat{y}}= \operatorname{argmax }~\operatorname{softmax}(\boldsymbol{\theta}^{\star} \mathbf{x}_{test}).\end{split}
\end{equation}

\subsubsection{Naïve Bayes}
\label{\detokenize{chapter3:naive-bayes}}
\sphinxAtStartPar
Dans cette sous section nous allons introduire un autre algorithme
souvent utilisé dans le cadre des problèmes de classification. Cet
algorithme est connu sous le nom de \sphinxstylestrong{Naïve Bayes}, naïve parce qu’il
fait une simple hypothèse sur les données, celle qui suppose qu’elles
sont indépendantes les une des autres, même si ceci n’est souvent pas
vrai en pratique.

\sphinxAtStartPar
Les systèmes modernes populaires comme la classification des émails
reçus comme \sphinxstylestrong{spam} ou \sphinxstylestrong{non\sphinxhyphen{}spam} sont souvent implémentés avec
naïves Bayes et quelques fois leur performance est difficile à surpasser
par des algorithmes sophistiqués.

\sphinxAtStartPar
Naïve Bayes fait partie des algorithmes de type \sphinxstylestrong{génératif}, qui sont
différents des algorithmes vus jusque\sphinxhyphen{}là qui sont de type
\sphinxstylestrong{discriminatif}.

\sphinxAtStartPar
L’une des grandes différences entre les algorithmes de type génératif et
ceux dits discriminatifs réside dans le fait que les premiers font une
hypothèse sur les données \(\mathbb{P}(\mathbf{x}|y)\) tandis que
les derniers font une hypothèse sur les étiquettes (classes)
\(\mathbb{P}(y|\mathbf{x})\).

\sphinxAtStartPar
Pour ceux qui sont familiers avec les notions mathématiques, peut être
que vous vous êtes posé la question si cet algorithme a un lien avec la
règle de Bayes (Naïve Bayes) ? OUI! vous avez raison car cette formule
nous donne un lien entre les algorithmes de type discriminatif et ceux
de type génératif.

\sphinxAtStartPar
Rappelons la formule {\hyperref[\detokenize{chapter3:naivebayes}]{\emph{{[}naivebayes{]}}}} (\autopageref*{\detokenize{chapter3:naivebayes}})
\begin{equation}\label{equation:chapter3:chapter3:49}
\begin{split}\begin{aligned}
    \mathbb{P}(\mathbf{x}|y) &= \frac{\mathbb{P}(y|\mathbf{x}) \mathbb{P}(\mathbf{x})}{\mathbb{P}(y)}.\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Considérons le cas de la classification des emails (spam ou no\sphinxhyphen{}spam).
Nous pouvons ré\sphinxhyphen{}écrire la formule précédente comme :
\begin{equation}\label{equation:chapter3:chapter3:50}
\begin{split}\begin{aligned}
    \mathbb{P}(\mbox{document} |\mbox{classe}) &= \frac{\mathbb{P}(\mbox{classe}|\mbox{document}) \mathbb{P}(\mbox{document})}{\mathbb{P}(\mbox{classe})}. \end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
Avec \sphinxstylestrong{document} qui représente l’ensemble des émails, et \sphinxstylestrong{classe}
leur catégorie.

\sphinxAtStartPar
Pour mieux illustrer la notion introduite dans le paragraphe précédent,
réfléchissons sur un exemple pratique. Disons que vous êtes responsable
d’un champ de feuille de manioc et vous voulez avoir un système qui vous
alarme dès qu’une chèvre entre dans le champ. Nous supposerons (non
réaliste) que dans cette contrée nous ne pouvons avoir que deux espèces
d’animaux, chèvre et chien, donc nous avons deux catégories
\(c_1\)=”chèvre” et \(c_2\) = “chien”.

\sphinxAtStartPar
Comme vous l’avez sûrement appris dans ce cours, les algorithmes que
nous utilisons en apprentissage automatique ne prennent en entrée que
les données de type numérique. Alors nos données deviennent :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathbf{x}\) qui représente certaines caractéristiques des deux
animaux, par exemple : ‘couleur’, ‘cris’, ‘vitesse’, …, toutes ces
caractéristiques ont une représentation numérique (cris : bêler=0,
aboyer=1, …).

\item {} 
\sphinxAtStartPar
\(y\) qui prend la valeur “0” pour une chèvre et “1” pour un
chien.

\end{enumerate}

\sphinxAtStartPar
Dans ce contexte, un algorithme de type discriminatif va chercher à
apprendre une application (mapping) de l’espace des \(\mathbf{x}\)
dans l’espace des étiquettes \(\{0, 1\}\); en d’autres termes, ce
type d’algorithmes va chercher à trouver une ligne (ou hyperplan) qui va
séparer les chiens des chèvres étant données les caractéristiques
(\(\mathbf{x}\)) observées, tandis que celui de type génératif va se
focaliser à la modélisation des caractéristiques qui distinguent un
chien d’une chèvre.

\begin{sphinxuseclass}{algorithm}
\begin{sphinxuseclass}{algorithmic}
\sphinxAtStartPar
\sphinxstylestrong{fonction} \(Entrainer\_Naïve\_Bayes(D,\ C)\):        \sphinxstylestrong{Pour
chaque} classe dans \(C\):        \(N_{doc}=\) nombre de
documents dans \(D\)        \(N_{c}=\) nombre de documents
de classe \(c\) dans \(D\)
\(\log \operatorname{Prob}[c]= \log\frac{N_c}{N_{doc}}\)
       \(V=\) vocabulaire de \(D\)
\(\operatorname{classedoc}[c]=\) \sphinxstylestrong{ajouter}(\(d\)) pour
tout \(d\in D\) avec pour classe \(c\)        \sphinxstylestrong{Pour
chaque} mot \(w\) de classe \(c \in C\) dans \(V\):
            \sphinxstylestrong{compter}(w, c) = nombre d’occurrence du mot
\(w\) dans \(\operatorname{classedoc}[c]\)
\(\log \operatorname{Prob\_wc} [w, c] \leftarrow \log \frac{\operatorname{compter}(w, c)}{\sum_{w^{\prime} \in V} \left(\text {compter}\left(w^{\prime}, c\right)\right)}\)
\sphinxstylestrong{retourner} \(\log \operatorname{Prob}\),
\(\log \operatorname{Prob\_wc},\ V\)

\end{sphinxuseclass}
\sphinxAtStartPar
{[}train\_naive{]}

\end{sphinxuseclass}
\begin{sphinxuseclass}{algorithm}
\begin{sphinxuseclass}{algorithmic}
\sphinxAtStartPar
\sphinxstylestrong{fonction}
\(Tester\_naive\_bayes(testdoc,\ logProb,\ logProb\_wc,\ C,\ V)\):
      \sphinxstylestrong{Pour chaque} classe \(c \in C\):
         \(\operatorname{somme[c]} = \log \operatorname{Prob}[c]\)
      \sphinxstylestrong{Pour chaque} position i dans \(testdoc\):
         \(\operatorname{mot} = \operatorname{testdoc[i]}\)
              \(\textbf{Si} \operatorname{mot} \in V:\)
                 \(\operatorname{somme[c]}=\operatorname{somme[c]}+\log\)
\(\operatorname{Prob\_wc}[mot, c]\)

\sphinxAtStartPar
       \sphinxstylestrong{retourner}
\(\underset{c}{\arg\max} \quad \operatorname{somme[c]}\)

\end{sphinxuseclass}
\sphinxAtStartPar
{[}test\_naive{]}

\end{sphinxuseclass}
\sphinxAtStartPar
Ainsi, parlons de comment nous pouvons construire un modèle de
classification en utilisant Naïve Bayes.


\paragraph{Entraînement du Naïve Bayes (Exemple Pratique)}
\label{\detokenize{chapter3:entrainement-du-naive-bayes-exemple-pratique}}
\sphinxAtStartPar
Considérons le cas de l’analyse de sentiments, sur base des commentaires
de gens après avoir suivi un film. Pour raison de simplicité nous
considérerons un exemple de quelques phrases et leurs classes
correspondantes.
\begin{equation}\label{equation:chapter3:chapter3:51}
\begin{split}\begin{array}{lll}
    \hline \text{Mode} & \text { Classe } & \multicolumn{1}{c} {\text { Documents }} \\
    \hline \text{ Train } & - & \text { tout simplement ennuyeux } \\
    & - & \text { tout à fait prévisible et manque d'énergie } \\
    & - & \text { pas de surprises et très peu de rires } \\
    & + & \text { très intéressant } \\
    & + & \text { le film le plus amusant de l'année } \\
    \hline \text { Test } & ? & \text { prévisible sans amusement }
    \end{array}\end{split}
\end{equation}
\sphinxAtStartPar
{[}fig:my\_label{]}

\sphinxAtStartPar
Nous commencerons par calculer le \sphinxstylestrong{à priori} pour les deux classes
comme élaboré dans l’algorithme {\hyperref[\detokenize{chapter3:train_naive}]{\emph{{[}train\_naive{]}}}} (\autopageref*{\detokenize{chapter3:train_naive}}).
\begin{equation}\label{equation:chapter3:chapter3:52}
\begin{split}\mathbb{P}(+) = \log \frac{N_{+}}{N_{doc}}= \log \frac{2}{5}= -0.916290731874155\end{split}
\end{equation}\begin{equation}\label{equation:chapter3:chapter3:53}
\begin{split}\mathbb{P}(-) = \log \frac{N_{-}}{N_{doc}} = \log \frac{3}{5} = -0.5108256237659907\end{split}
\end{equation}
\sphinxAtStartPar
Comme vous l’avez remarqué, les formules que nous utilisons pour notre
algorithme sont dans l’échelle logarithmique, ceci, pour raison d’éviter
ce qui est connu en anglais comme underflow (quand les valeurs sont très
proches de zéro au point d’être vue par l’ordinateur comme zéro),
overflow (quand les valeurs sont très grandes) mais aussi pour augmenter
la vitesse de calcul.

\sphinxAtStartPar
La prochaine étape consiste à définir notre vocabulaire \(V\). Le
\(V\) est un ensemble qui contient les mots uniques de nos données.

\sphinxAtStartPar
V = \{‘tout’, ‘simplement’, ‘ennuyeux’, ‘à’, ‘fait’, ‘prévisible’, ‘et’,
‘manque’, ‘d’, ‘énergie’, ‘pas’, ‘de’, ‘surprises’, ‘très’, ‘peu’,
‘rires’, ‘intéressant’, ‘le’, ‘film’, ‘plus’, ‘amusant’, ‘l’, ‘année’\}.

\sphinxAtStartPar
Nous avons au total \(23\) mots uniques. Nous allons par la suite,
calculer les valeurs de \(classedoc\) pour les deux classes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\operatorname{classedoc}[+]\) = {[}“très intéressant”, “le film
le plus amusant de l’année”{]}

\item {} 
\sphinxAtStartPar
\(\operatorname{classedoc}[-]\) = {[}“tout simplement ennuyeux”,
“tout à fait prévisible et manque d’énergie”, “pas de surprises et
très peu de rires”{]}.

\end{itemize}

\sphinxAtStartPar
Calculons ensuite le logarithme de la probabilité (d’apparition) de
chaque mot dans notre vocabulaire étant donné une classe particulière.
En pratique, pour le calcul de \(\log \operatorname{Prob\_wc}\), il
arrive que nous rencontrons de nouveaux mots qui n’étaient pas présents
dans l’étape d’entraînement, ceci conduit au fait que
\(\log \operatorname{Prob\_wc} = \log(0)\) qui n’est pas défini.
Alors pour contourner ce problème, plusieurs alternatives existent dans
la littérature; dans le contexte de notre exemple, nous allons utiliser
la technique appelée “add\sphinxhyphen{}one (Laplace) smothing” qui va transformer la
formule de \(\log \operatorname{Prob}\_wc\) fournie dans
l’algorithme {\hyperref[\detokenize{chapter3:train_naive}]{\emph{{[}train\_naive{]}}}} (\autopageref*{\detokenize{chapter3:train_naive}}):
\begin{equation}\label{equation:chapter3:chapter3:54}
\begin{split}\begin{aligned}
\displaystyle \log \operatorname{Prob}\_wc &= \log \frac{\operatorname{compter}(w, c)+1}{\sum_{w^{\prime} \in V} \left(\text {compter}\left(w^{\prime}, c\right)+1\right)}\\
&=\log \frac{\operatorname{compter}(w, c)+1}{\sum_{w^{\prime} \in V} \left(\operatorname {compter}\left(w^{\prime}, c\right)\right)+|V|}\end{aligned}\end{split}
\end{equation}
\sphinxAtStartPar
En réalité comme vous l’avez peut être remarqué, l’algorithme de Naïve
Bayes est simplement un comptage systématique des mots.

\sphinxAtStartPar
Notre tâche ici est de classifier la phrase “\sphinxstyleemphasis{prévisible sans
amusement}” comme soit positive (+) ou négative (\sphinxhyphen{}).

\begin{sphinxuseclass}{center}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Sentiment} \PYG{n}{positive} \PYG{p}{(}\PYG{o}{+}\PYG{p}{)}                                                         \PYG{n}{Sentiment} \PYG{n}{négative} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}

\bigskip\hrule\bigskip



\subsection{Commentaires et Discussions}
\label{\detokenize{chapter3:commentaires-et-discussions}}
\sphinxAtStartPar
Partagez vos questions, commentaires et expériences avec la communauté
IVIA\sphinxhyphen{}AF ! Utilisez la version en ligne
\sphinxhref{https://livre.ivia.africa/chapter3.html}{livre.ivia.africa}%
\begin{footnote}[11]\sphinxAtStartFootnote
\sphinxnolinkurl{https://livre.ivia.africa/chapter3.html}
%
\end{footnote}.

\sphinxAtStartPar
\sphinxstyleemphasis{Les commentaires sont modérés pour maintenir un environnement
d’apprentissage respectueux et constructif.}

\sphinxstepscope


\section{Bibliography}
\label{\detokenize{bibliography:bibliography}}\label{\detokenize{bibliography::doc}}
\sphinxAtStartPar
The breakthrough of deep learning origins from
(\hyperlink{cite.bibliography:id9}{Krizhevsky \sphinxstyleemphasis{et al.}, 2017}) for computer vision, there is a rich of
following up works, such as (\hyperlink{cite.bibliography:id10}{He \sphinxstyleemphasis{et al.}, 2016}). NLP is catching up as
well, the recent work (\hyperlink{cite.bibliography:id8}{Devlin \sphinxstyleemphasis{et al.}, 2018}) shows significant
improvements.

\sphinxAtStartPar
Two keys together (\hyperlink{cite.bibliography:id8}{Devlin \sphinxstyleemphasis{et al.}, 2018}, \hyperlink{cite.bibliography:id10}{He \sphinxstyleemphasis{et al.}, 2016}). Single author
, two authors Newell and Rosenbloom (\hyperlink{cite.bibliography:id12}{1980})


\bigskip\hrule\bigskip



\section{Commentaires et Discussions}
\label{\detokenize{index:commentaires-et-discussions}}
\sphinxAtStartPar
Partagez vos questions, commentaires et expériences avec la communauté
IVIA\sphinxhyphen{}AF ! Utilisez la version en ligne
\sphinxhref{https://livre.ivia.africa/}{livre.ivia.africa}%
\begin{footnote}[12]\sphinxAtStartFootnote
\sphinxnolinkurl{https://livre.ivia.africa/}
%
\end{footnote}.

\sphinxAtStartPar
\sphinxstyleemphasis{Les commentaires sont modérés pour maintenir un environnement
d’apprentissage respectueux et constructif.}

\begin{sphinxthebibliography}{Newell \&}
\bibitem[Devlin et al., 2018]{bibliography:id8}
\sphinxAtStartPar
Devlin, J., Chang, M.\sphinxhyphen{}W., Lee, K., \& Toutanova, K. (2018). Bert: pre\sphinxhyphen{}training of deep bidirectional transformers for language understanding. \sphinxstyleemphasis{arXiv preprint arXiv:1810.04805}.
\bibitem[He et al., 2016]{bibliography:id10}
\sphinxAtStartPar
He, K., Zhang, X., Ren, S., \& Sun, J. (2016). Deep residual learning for image recognition. \sphinxstyleemphasis{Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 770–778).
\bibitem[Krizhevsky et al., 2017]{bibliography:id9}
\sphinxAtStartPar
Krizhevsky, A., Sutskever, I., \& Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. \sphinxstyleemphasis{Communications of the ACM}, \sphinxstyleemphasis{60}(6), 84–90.
\bibitem[Newell \& Rosenbloom, 1980]{bibliography:id12}
\sphinxAtStartPar
Newell, A., \& Rosenbloom, P. S. (1980). \sphinxstyleemphasis{Mechanisms of skill acquisition and the law of practice.} CARNEGIE\sphinxhyphen{}MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE.
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}